{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "str = 'Helloworld!!' # l = 12\n",
    "class unit():\n",
    "    def __init__(self, left, right):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "    def printt(self):\n",
    "        if type(self.left) == unit:\n",
    "            self.left.printt()\n",
    "        else:\n",
    "            print(f'{self.left['symbol']}, ')\n",
    "            \n",
    "        if type(self.right) == unit:\n",
    "            self.right.printt()\n",
    "        else:\n",
    "            print(f'{self.right['symbol']}, ')\n",
    "            \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vector_quantize_pytorch.vector_quantize_pytorch import VectorQuantize\n",
    "from vector_quantize_pytorch.residual_vq import ResidualVQ, MultiLayerVQ\n",
    "\n",
    "from compressai.models.utils import conv, deconv, update_registered_buffers\n",
    "from compressai.layers import GDN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import warnings\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from compressai.models import ScaleHyperprior\n",
    "from compressai.entropy_models import EntropyBottleneck, GaussianConditional\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "from torch import Tensor\n",
    "\n",
    "from typing import Any, Callable, List, Optional, Tuple, Union\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from compressai.datasets import ImageFolder\n",
    "import torch.optim as optim\n",
    "import shutil\n",
    "from pytorch_msssim import ms_ssim\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "from compressai.layers import (\n",
    "    AttentionBlock,\n",
    "    ResidualBlock,\n",
    "    ResidualBlockUpsample,\n",
    "    ResidualBlockWithStride,\n",
    "    conv3x3,\n",
    "    subpel_conv3x3,\n",
    ")\n",
    "from compressai.ops import LowerBound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_enable = True\n",
    "ce_loss_enable = False\n",
    "param_direct = True\n",
    "lossless_param = True\n",
    "\n",
    "log_path = \"log\"\n",
    "version = \"test\"\n",
    "lambda_setting = 0.003\n",
    "quantizers = 1\n",
    "CB_size_setting = 512\n",
    "dim = 64\n",
    "workers_setting = 4\n",
    "batch_size_setting = 8\n",
    "epochs = 1000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hyper_VQ(ScaleHyperprior):\n",
    "    def __init__(self, N=128, quantizers=1, CB_size=512, dim=64, **kwargs):\n",
    "        super().__init__(N, N, **kwargs)\n",
    "        self.gaussian_conditional = GaussianConditional(None)\n",
    "\n",
    "        self.lower_bound_l = LowerBound(1e-9)\n",
    "        self.lower_bound_s = LowerBound(0.11)\n",
    "\n",
    "        self.g_a = nn.Sequential(\n",
    "            ResidualBlockWithStride(3, N, stride=2),\n",
    "            AttentionBlock(N),\n",
    "            ResidualBlock(N, N),\n",
    "            ResidualBlockWithStride(N, N, stride=2),\n",
    "            AttentionBlock(N),\n",
    "            ResidualBlock(N, N),\n",
    "            conv3x3(N, dim, stride=2),\n",
    "            AttentionBlock(dim),\n",
    "            ResidualBlock(dim, dim),\n",
    "        )\n",
    "        self.h_a = nn.Sequential(\n",
    "            conv3x3(2, N),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            conv3x3(N, N),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            conv3x3(N, N, stride=2),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            conv3x3(N, N),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            conv3x3(N, N, stride=2),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            conv3x3(N, N),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            conv3x3(N, N),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            conv3x3(N, N, stride=2),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            conv3x3(N, N),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            conv3x3(N, N),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            conv3x3(N, N, stride=2),\n",
    "        )\n",
    "        self.h_s = nn.Sequential(\n",
    "            conv3x3(N, N),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            subpel_conv3x3(N, N, 2),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            conv3x3(N, N),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            subpel_conv3x3(N, N, 2),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            conv3x3(N, N),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            subpel_conv3x3(N, N, 2),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            conv3x3(N, N),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            subpel_conv3x3(N, N, 2),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            conv3x3(N, 2),\n",
    "        )\n",
    "        self.g_s = nn.Sequential(\n",
    "            AttentionBlock(dim),\n",
    "            ResidualBlock(dim, dim),\n",
    "            ResidualBlockUpsample(dim, N, 2),\n",
    "            AttentionBlock(N),\n",
    "            ResidualBlock(N, N),\n",
    "            ResidualBlockUpsample(N, N, 2),\n",
    "            AttentionBlock(N),\n",
    "            ResidualBlock(N, N),\n",
    "            subpel_conv3x3(N, 3, 2),\n",
    "        )\n",
    "        if quantizers == 1:\n",
    "            self.vq = VectorQuantize(\n",
    "                dim = dim,\n",
    "                codebook_size = CB_size,    # codebook size\n",
    "                decay = 0.99,               # the exponential moving average decay, lower means the dictionary will change faster\n",
    "                commitment_weight = 0.25,   # the weight on the commitment loss\n",
    "                accept_image_fmap = True,\n",
    "            )\n",
    "        elif quantizers > 1:\n",
    "            self.vq = MultiLayerVQ(\n",
    "                num_quantizers = quantizers,\n",
    "                dim = dim // quantizers,\n",
    "                codebook_size = CB_size,    # codebook size\n",
    "                decay = 0.99,               # the exponential moving average decay, lower means the dictionary will change faster\n",
    "                commitment_weight = 0.25,   # the weight on the commitment loss\n",
    "                accept_image_fmap = True,\n",
    "            )\n",
    "    def calc_cross_entropy(self, symbol):\n",
    "        ones = torch.ones_like(symbol).to(device).float()\n",
    "        cross_entropy_from_N_01 = self.gaussian_conditional._likelihood(symbol, ones)\n",
    "        cross_entropy_from_N_01 = self.lower_bound_l(cross_entropy_from_N_01)\n",
    "        return cross_entropy_from_N_01\n",
    "\n",
    "    def standardized(self, y, means, scales):\n",
    "        variance = self.lower_bound_s(scales)\n",
    "        y_std = y - means\n",
    "        y_std = y_std / variance\n",
    "        ce_N01 = self.calc_cross_entropy(y_std)\n",
    "        return y_std, ce_N01\n",
    "        \n",
    "    def destandardized(self, y_std, means, scales):\n",
    "        variance = self.lower_bound_s(scales)\n",
    "        y_ = y_std * variance\n",
    "        y_ = y_ + means\n",
    "        return y_\n",
    "\n",
    "# x = torch.randn(1, 4, 2, 2)\n",
    "# x_ = x.mean(1).unsqueeze(1)\n",
    "# xx = (x - x_) ** 2\n",
    "# xx_ = xx.mean(1).unsqueeze(1)    \n",
    "    \n",
    "    \n",
    "    def represent_befor_quantize(self, y):        \n",
    "        z = self.h_a(y)\n",
    "        z_hat, z_likelihoods = self.entropy_bottleneck(z)\n",
    "        gaussian_params = self.h_s(z_hat)\n",
    "        scales_hat, means_hat = gaussian_params.chunk(2, 1)\n",
    "        y_std, ce = self.standardized(y, means_hat, scales_hat)\n",
    "        return y_std, ce, z_likelihoods, scales_hat, means_hat\n",
    "\n",
    "    def direct_calc_gaussian(self, y):\n",
    "        means = y.mean(1).unsqueeze(1)\n",
    "        scales = ((y - means) ** 2).mean(1).unsqueeze(1)        \n",
    "        variance = torch.sqrt(scales)\n",
    "        gaussian_params = torch.cat([variance, means], 1)\n",
    "        z = self.h_a(gaussian_params)\n",
    "        z_hat, z_likelihoods = self.entropy_bottleneck(z)\n",
    "        gp_hat = self.h_s(z_hat)\n",
    "        scales_hat, means_hat = gp_hat.chunk(2, 1)\n",
    "        if lossless_param:\n",
    "            y_std, ce = self.standardized(y, means, variance)            \n",
    "        else:\n",
    "            y_std, ce = self.standardized(y, means_hat, scales_hat)\n",
    "        return y_std, ce, z_likelihoods, scales_hat, means_hat, gaussian_params, gp_hat\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.g_a(x)\n",
    "        if hyper_enable and not param_direct:\n",
    "            y_std, ce, z_likelihoods, scales_hat, means_hat = self.represent_befor_quantize(y)\n",
    "        elif hyper_enable and param_direct:\n",
    "            y_std, ce, z_likelihoods, scales_hat, means_hat, gp, gp_hat = self.direct_calc_gaussian(y)            \n",
    "        else:\n",
    "            y_std = y\n",
    "            ce = self.calc_cross_entropy(y)\n",
    "            z_likelihoods = torch.tensor(0)\n",
    "        \n",
    "        y_hat, id, commit, usage = self.vq(y_std)  # (b, Q, w, h), (b, Q, w, h), (b), (b)\n",
    "        y_hat_ = self.destandardized(y_hat, scales_hat, means_hat) if hyper_enable else y_hat\n",
    "        x_hat = self.g_s(y_hat_)\n",
    "\n",
    "        return {\n",
    "            \"x_hat\": x_hat,\n",
    "            \"likelihoods\": {\"z\": z_likelihoods},\n",
    "            \"commit\": commit,\n",
    "            \"usage\": usage,\n",
    "            \"cross\": ce,\n",
    "            \"gp\": gp,\n",
    "            \"gp_hat\": gp_hat,            \n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_psnr(a, b):\n",
    "    mse = torch.mean((a - b)**2).item()\n",
    "    return -10 * math.log10(mse)\n",
    "\n",
    "def compute_mse(a, b):\n",
    "    mse = torch.mean((a - b)**2).item()\n",
    "    return mse\n",
    "\n",
    "def compute_msssim(a, b):\n",
    "    return ms_ssim(a, b, data_range=1.).item()\n",
    "\n",
    "def compute_bpp(out_net):\n",
    "    size = out_net['x_hat'].size()\n",
    "    num_pixels = size[0] * size[2] * size[3]\n",
    "    return sum(torch.log(likelihoods).sum() / (-math.log(2) * num_pixels)\n",
    "              for likelihoods in out_net['likelihoods'].values()).item()\n",
    "\n",
    "# def configure_optimizers(net):\n",
    "#     \"\"\"Separate parameters for the main optimizer and the auxiliary optimizer.\n",
    "#     Return two optimizers\"\"\"\n",
    "\n",
    "#     parameters = {\n",
    "#         n\n",
    "#         for n, p in net.named_parameters()\n",
    "#         if not n.endswith(\".quantiles\") and p.requires_grad\n",
    "#     }\n",
    "#     aux_parameters = {\n",
    "#         n\n",
    "#         for n, p in net.named_parameters()\n",
    "#         if n.endswith(\".quantiles\") and p.requires_grad\n",
    "#     }\n",
    "\n",
    "#     # Make sure we don't have an intersection of parameters\n",
    "#     params_dict = dict(net.named_parameters())\n",
    "#     inter_params = parameters & aux_parameters\n",
    "#     union_params = parameters | aux_parameters\n",
    "\n",
    "#     assert len(inter_params) == 0\n",
    "#     assert len(union_params) - len(params_dict.keys()) == 0\n",
    "\n",
    "#     optimizer = optim.Adam(\n",
    "#         (params_dict[n] for n in sorted(parameters)),\n",
    "#         lr=1e-4,\n",
    "#     )\n",
    "#     aux_optimizer = optim.Adam(\n",
    "#         (params_dict[n] for n in sorted(aux_parameters)),\n",
    "#         lr=1e-3,\n",
    "#     )\n",
    "#     return optimizer, aux_optimizer\n",
    "\n",
    "class AverageMeter:\n",
    "    \"\"\"Compute running average.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        \n",
    "def configure_optimizers(net):\n",
    "    \"\"\"Separate parameters for the main optimizer and the auxiliary optimizer.\n",
    "    Return two optimizers\"\"\"\n",
    "    parameters = {\n",
    "        n\n",
    "        for n, p in net.named_parameters()\n",
    "        if not n.endswith(\".quantiles\") and p.requires_grad and not n.startswith(\"h\")\n",
    "    }\n",
    "    aux_parameters = {\n",
    "        n\n",
    "        for n, p in net.named_parameters()\n",
    "        if n.endswith(\".quantiles\") and p.requires_grad\n",
    "    }\n",
    "    hyper_parameters = {\n",
    "            n\n",
    "        for n, p in net.named_parameters()\n",
    "        if n.startswith(\"h\") and p.requires_grad\n",
    "    }\n",
    "\n",
    "    # Make sure we don't have an intersection of parameters\n",
    "    params_dict = dict(net.named_parameters())\n",
    "    inter_params = parameters & aux_parameters & hyper_parameters\n",
    "    union_params = parameters | aux_parameters | hyper_parameters\n",
    "\n",
    "    assert len(inter_params) == 0\n",
    "    assert len(union_params) - len(params_dict.keys()) == 0\n",
    "\n",
    "    optimizer = optim.Adam(\n",
    "        (params_dict[n] for n in sorted(parameters)),\n",
    "        lr=1e-4,\n",
    "    )\n",
    "    hyper_optimizer = optim.Adam(\n",
    "        (params_dict[n] for n in sorted(hyper_parameters)),\n",
    "        lr=1e-4,\n",
    "    )\n",
    "    aux_optimizer = optim.Adam(\n",
    "        (params_dict[n] for n in sorted(aux_parameters)),\n",
    "        lr=1e-3,\n",
    "    )\n",
    "    return optimizer, hyper_optimizer, aux_optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function, Train, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "class RateDistortionLoss(nn.Module):\n",
    "    \"\"\"Custom rate distortion loss with a Lagrangian parameter.\"\"\"\n",
    "\n",
    "    def __init__(self, lmbda=1e-2):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.lmbda = lmbda\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        N, _, H, W = target.size()\n",
    "        out = {}\n",
    "        num_pixels = N * H * W\n",
    "        out[\"mse_loss\"] = self.mse(output[\"x_hat\"], target)\n",
    "        out[\"commit\"] = output[\"commit\"]\n",
    "        out[\"perplexity\"] = output[\"usage\"]\n",
    "        \n",
    "        out[\"hyper_loss\"] = F.mse_loss(output['gp'], output['gp_hat'])        \n",
    "\n",
    "        if hyper_enable:        \n",
    "            out[\"bpp_loss\"] = sum(\n",
    "                (torch.log(likelihoods).sum() / (-math.log(2) * num_pixels))\n",
    "                for likelihoods in output[\"likelihoods\"].values()\n",
    "            )\n",
    "        else:\n",
    "            out[\"bpp_loss\"] = 0\n",
    "        out[\"ce_loss\"] = torch.log(output[\"cross\"]).sum() / (-math.log(2) * num_pixels)\n",
    "        if ce_loss_enable:\n",
    "            out[\"loss\"] = self.lmbda * 255**2 *  out[\"mse_loss\"] + out[\"commit\"].sum() + out[\"ce_loss\"]\n",
    "        else:\n",
    "            out[\"loss\"] = (self.lmbda * 255**2) * (out[\"mse_loss\"] + out[\"hyper_loss\"]) + out[\"commit\"].sum()\n",
    "        return out\n",
    "\n",
    "def train_one_epoch(\n",
    "    model, criterion, train_dataloader, optimizer, hyper_optimizer, aux_optimizer, epoch, clip_max_norm\n",
    "):\n",
    "    model.train()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    for i, d in enumerate(train_dataloader):\n",
    "        d = d.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        hyper_optimizer.zero_grad()\n",
    "        aux_optimizer.zero_grad()\n",
    "        out_net = model(d)\n",
    "        \n",
    "        out_criterion = criterion(out_net, d)\n",
    "        out_criterion[\"loss\"].backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        \n",
    "#         if param_direct:\n",
    "#             hyper_loss = F.mse_loss(out_net['gp'], out_net['gp_hat'])\n",
    "#             hyper_loss.backward()\n",
    "        if hyper_enable:\n",
    "            hyper_optimizer.step()\n",
    "        aux_loss = model.aux_loss()\n",
    "        aux_loss.backward()\n",
    "        aux_optimizer.step()\n",
    "\n",
    "        if i == 100:\n",
    "            print(\n",
    "                f\"Train epoch {epoch}: [\"\n",
    "                f\"{i*len(d)}/{len(train_dataloader.dataset)}\"\n",
    "                f\" ({100. * i / len(train_dataloader):.0f}%)]\"\n",
    "                f'\\t Loss: {out_criterion[\"loss\"].item():.3f} |'\n",
    "                f'\\t MSE loss: {out_criterion[\"mse_loss\"].item():.3f} |'\n",
    "                f'\\t Bpp loss: {out_criterion[\"bpp_loss\"].item():.2f} |'                \n",
    "                f'\\t vq commit:{out_criterion[\"commit\"].sum().item():.2f} |'\n",
    "                f'\\t perplexity:{out_criterion[\"perplexity\"].mean().item():.2f} |'\n",
    "                f\"\\t Aux loss: {aux_loss.item():.2f}\"\n",
    "            )\n",
    "            \n",
    "def test_epoch(epoch, test_dataloader, model:Hyper_VQ, criterion):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    loss = AverageMeter()\n",
    "    psnr_score = AverageMeter()\n",
    "    mse_loss = AverageMeter()\n",
    "    bpp_loss = AverageMeter()\n",
    "    aux_loss = AverageMeter()\n",
    "    y_mse = AverageMeter()\n",
    "    commit_loss = AverageMeter()\n",
    "    usage_status = AverageMeter()\n",
    "    CE_loss = AverageMeter()\n",
    "    \n",
    "    hyper_mse_loss = AverageMeter()\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for d in test_dataloader:\n",
    "            d = d.to(device)\n",
    "\n",
    "            y = model.g_a(d)\n",
    "            if hyper_enable and not param_direct:\n",
    "                y_std, ce, z_likelihoods, scales_hat, means_hat = model.represent_befor_quantize(y)    \n",
    "            elif hyper_enable and param_direct:\n",
    "                y_std, ce, z_likelihoods, scales_hat, means_hat, gp, gp_hat = model.direct_calc_gaussian(y)\n",
    "            else:\n",
    "                y_std = y\n",
    "                ce = model.calc_cross_entropy(y)\n",
    "                z_likelihoods = torch.tensor(0)\n",
    "            \n",
    "            y_hat, id, commit, usage = model.vq(y_std)  # (b, Q, w, h), (b, Q, w, h), (b), (b)\n",
    "            y_hat_ = model.destandardized(y_hat, scales_hat, means_hat) if hyper_enable else y_hat\n",
    "            x_hat = model.g_s(y_hat_)\n",
    "\n",
    "            out_net = {\n",
    "                \"x_hat\": x_hat,\n",
    "                \"likelihoods\": {\"z\": z_likelihoods},\n",
    "                \"commit\": commit,\n",
    "                \"usage\": usage,\n",
    "                \"cross\": ce,\n",
    "                \n",
    "                \"gp\": gp,\n",
    "                \"gp_hat\": gp_hat,            \n",
    "                                \n",
    "            }\n",
    "\n",
    "            out_criterion = criterion(out_net, d)\n",
    "            loss.update(out_criterion[\"loss\"])\n",
    "            psnr_score.update(compute_psnr(x_hat, d))\n",
    "            mse_loss.update(out_criterion[\"mse_loss\"])\n",
    "            bpp_loss.update(out_criterion[\"bpp_loss\"])\n",
    "            aux_loss.update(model.aux_loss())\n",
    "            y_criterion = compute_mse(y_hat_, y)\n",
    "            y_mse.update(y_criterion)\n",
    "            commit_loss.update(out_criterion[\"commit\"].sum())\n",
    "            usage_status.update(out_criterion[\"perplexity\"])\n",
    "            CE_loss.update(out_criterion[\"ce_loss\"])\n",
    "\n",
    "            hyper_mse_loss.update(F.mse_loss(gp_hat, gp))\n",
    "            \n",
    "            \n",
    "    print(\n",
    "        f\"{epoch}: \"\n",
    "        f\"\\tLoss: {loss.avg:.3f} |\"\n",
    "        f\"\\tPSNR: {psnr_score.avg:.3f} |\"\n",
    "        f\"\\tMSE: {mse_loss.avg:.3f} |\"\n",
    "        f\"\\ty_mse : {y_mse.avg:.2f} |\"\n",
    "        f\"\\tcommit : {commit_loss.avg:.2f} |\"\n",
    "        f\"\\tusage : {usage_status.avg:.2f} |\"\n",
    "        f\"\\tCEL : {CE_loss.avg:.2f} |\"\n",
    "        f\"\\tBpp: {bpp_loss.avg:.2f} |\"\n",
    "        f\"\\tH_mse: {hyper_mse_loss.avg:.4f} |\"\n",
    "    )\n",
    "    with open(f'{log_path}.csv','a') as f:\n",
    "        log_s = f\"{epoch}, {psnr_score.avg}, {mse_loss.avg}, {y_mse.avg}, {commit_loss.avg}, {usage_status.avg}, {CE_loss.avg}, {bpp_loss.avg}, {hyper_mse_loss.avg}\\n\"\n",
    "        f.write(log_s)\n",
    "    return loss.avg\n",
    "\n",
    "def eval_image(net):\n",
    "    device = next(net.parameters()).device\n",
    "    img = Image.open('../assets/kodim15.png').convert('RGB')\n",
    "    x = transforms.ToTensor()(img).unsqueeze(0).to(device = device)\n",
    "    with torch.no_grad():\n",
    "        out_net = net.forward(x)\n",
    "    out_net['x_hat'].clamp_(0, 1)\n",
    "    psnr = compute_psnr(x, out_net[\"x_hat\"])\n",
    "    msssim = compute_msssim(x, out_net[\"x_hat\"])\n",
    "    if 'likelihoods' in out_net:\n",
    "        bitrate = compute_bpp(out_net)\n",
    "    else:\n",
    "        bitrate = 0\n",
    "    log_s = f\"{psnr}, {msssim}, {bitrate}\\n\"\n",
    "    print(log_s)\n",
    "    torchvision.utils.save_image(out_net['x_hat'], f'{version}.png')\n",
    "    return psnr, msssim, bitrate\n",
    "\n",
    "def save_checkpoint(state, is_best, filename=f'{version}.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, \"best.vq.tar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose(\n",
    "    [transforms.RandomCrop((256, 256)), transforms.ToTensor()]\n",
    ")\n",
    "\n",
    "test_transforms = transforms.Compose(\n",
    "    [transforms.ToTensor()]\n",
    ")\n",
    "\n",
    "train_dataset = ImageFolder('../../../DIV2K_HR', split=\"train\", transform=train_transforms)\n",
    "test_dataset = ImageFolder('../../../DIV2K_HR', split=\"test\", transform=test_transforms)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size = batch_size_setting,\n",
    "    num_workers = workers_setting,\n",
    "    shuffle=True,\n",
    "    pin_memory=(device == \"cuda\"),\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size = 18,\n",
    "    num_workers = workers_setting,\n",
    "    shuffle=False,\n",
    "    pin_memory=(device == \"cuda\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.0001\n",
      "0: \tLoss: 3.923 |\tPSNR: 17.186 |\tMSE: 0.019 |\ty_mse : 0.01 |\tcommit : 0.09 |\tusage : 0.01 |\tCEL : 1.99 |\tBpp: 0.04 |\tH_mse: 0.0005 |\n",
      "Learning rate: 0.0001\n",
      "1: \tLoss: 3.556 |\tPSNR: 17.577 |\tMSE: 0.017 |\ty_mse : 0.01 |\tcommit : 0.09 |\tusage : 0.01 |\tCEL : 2.02 |\tBpp: 0.04 |\tH_mse: 0.0003 |\n",
      "Learning rate: 0.0001\n",
      "2: \tLoss: 3.873 |\tPSNR: 17.274 |\tMSE: 0.019 |\ty_mse : 0.01 |\tcommit : 0.12 |\tusage : 0.01 |\tCEL : 2.03 |\tBpp: 0.04 |\tH_mse: 0.0005 |\n",
      "Learning rate: 0.0001\n",
      "3: \tLoss: 3.841 |\tPSNR: 17.226 |\tMSE: 0.019 |\ty_mse : 0.01 |\tcommit : 0.10 |\tusage : 0.01 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0002 |\n",
      "Learning rate: 0.0001\n",
      "4: \tLoss: 5.696 |\tPSNR: 15.761 |\tMSE: 0.027 |\ty_mse : 0.01 |\tcommit : 0.12 |\tusage : 0.01 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0021 |\n",
      "Learning rate: 0.0001\n",
      "5: \tLoss: 5.490 |\tPSNR: 15.670 |\tMSE: 0.027 |\ty_mse : 0.01 |\tcommit : 0.12 |\tusage : 0.01 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0004 |\n",
      "Learning rate: 0.0001\n",
      "6: \tLoss: 3.661 |\tPSNR: 17.505 |\tMSE: 0.018 |\ty_mse : 0.01 |\tcommit : 0.11 |\tusage : 0.01 |\tCEL : 2.04 |\tBpp: 0.04 |\tH_mse: 0.0004 |\n",
      "Learning rate: 0.0001\n",
      "7: \tLoss: 3.537 |\tPSNR: 17.624 |\tMSE: 0.017 |\ty_mse : 0.01 |\tcommit : 0.12 |\tusage : 0.01 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0003 |\n",
      "Learning rate: 0.0001\n",
      "8: \tLoss: 5.447 |\tPSNR: 15.669 |\tMSE: 0.027 |\ty_mse : 0.01 |\tcommit : 0.11 |\tusage : 0.01 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0002 |\n",
      "Learning rate: 0.0001\n",
      "9: \tLoss: 4.137 |\tPSNR: 16.913 |\tMSE: 0.020 |\ty_mse : 0.01 |\tcommit : 0.12 |\tusage : 0.01 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0003 |\n",
      "Learning rate: 0.0001\n",
      "10: \tLoss: 4.875 |\tPSNR: 16.210 |\tMSE: 0.024 |\ty_mse : 0.01 |\tcommit : 0.12 |\tusage : 0.01 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0004 |\n",
      "Learning rate: 0.0001\n",
      "11: \tLoss: 2.180 |\tPSNR: 19.754 |\tMSE: 0.011 |\ty_mse : 0.00 |\tcommit : 0.05 |\tusage : 0.04 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0003 |\n",
      "Learning rate: 0.0001\n",
      "12: \tLoss: 1.700 |\tPSNR: 20.796 |\tMSE: 0.008 |\ty_mse : 0.00 |\tcommit : 0.04 |\tusage : 0.09 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0002 |\n",
      "Learning rate: 0.0001\n",
      "13: \tLoss: 1.651 |\tPSNR: 20.907 |\tMSE: 0.008 |\ty_mse : 0.00 |\tcommit : 0.04 |\tusage : 0.13 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 0.0001\n",
      "14: \tLoss: 1.524 |\tPSNR: 21.260 |\tMSE: 0.007 |\ty_mse : 0.00 |\tcommit : 0.04 |\tusage : 0.15 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 0.0001\n",
      "15: \tLoss: 1.556 |\tPSNR: 21.177 |\tMSE: 0.008 |\ty_mse : 0.00 |\tcommit : 0.04 |\tusage : 0.18 |\tCEL : 2.04 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 0.0001\n",
      "16: \tLoss: 1.394 |\tPSNR: 21.687 |\tMSE: 0.007 |\ty_mse : 0.00 |\tcommit : 0.05 |\tusage : 0.23 |\tCEL : 2.04 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 0.0001\n",
      "17: \tLoss: 1.380 |\tPSNR: 21.743 |\tMSE: 0.007 |\ty_mse : 0.00 |\tcommit : 0.05 |\tusage : 0.25 |\tCEL : 2.04 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 0.0001\n",
      "18: \tLoss: 1.536 |\tPSNR: 21.234 |\tMSE: 0.008 |\ty_mse : 0.00 |\tcommit : 0.04 |\tusage : 0.23 |\tCEL : 2.04 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 0.0001\n",
      "19: \tLoss: 1.328 |\tPSNR: 21.903 |\tMSE: 0.006 |\ty_mse : 0.00 |\tcommit : 0.05 |\tusage : 0.29 |\tCEL : 2.04 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 0.0001\n",
      "20: \tLoss: 1.362 |\tPSNR: 21.783 |\tMSE: 0.007 |\ty_mse : 0.00 |\tcommit : 0.05 |\tusage : 0.27 |\tCEL : 2.04 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 0.0001\n",
      "21: \tLoss: 1.732 |\tPSNR: 20.712 |\tMSE: 0.008 |\ty_mse : 0.00 |\tcommit : 0.05 |\tusage : 0.31 |\tCEL : 2.04 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 0.0001\n",
      "22: \tLoss: 1.347 |\tPSNR: 21.828 |\tMSE: 0.007 |\ty_mse : 0.00 |\tcommit : 0.05 |\tusage : 0.33 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 0.0001\n",
      "23: \tLoss: 1.319 |\tPSNR: 21.936 |\tMSE: 0.006 |\ty_mse : 0.00 |\tcommit : 0.05 |\tusage : 0.38 |\tCEL : 2.04 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 0.0001\n",
      "24: \tLoss: 1.271 |\tPSNR: 22.109 |\tMSE: 0.006 |\ty_mse : 0.00 |\tcommit : 0.05 |\tusage : 0.36 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 0.0001\n",
      "25: \tLoss: 1.248 |\tPSNR: 22.186 |\tMSE: 0.006 |\ty_mse : 0.00 |\tcommit : 0.05 |\tusage : 0.39 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 0.0001\n",
      "26: \tLoss: 1.231 |\tPSNR: 22.267 |\tMSE: 0.006 |\ty_mse : 0.00 |\tcommit : 0.05 |\tusage : 0.41 |\tCEL : 2.04 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 0.0001\n",
      "27: \tLoss: 1.184 |\tPSNR: 22.438 |\tMSE: 0.006 |\ty_mse : 0.00 |\tcommit : 0.05 |\tusage : 0.44 |\tCEL : 2.04 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 0.0001\n",
      "28: \tLoss: 1.216 |\tPSNR: 22.295 |\tMSE: 0.006 |\ty_mse : 0.00 |\tcommit : 0.05 |\tusage : 0.40 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 0.0001\n",
      "29: \tLoss: 1.310 |\tPSNR: 21.961 |\tMSE: 0.006 |\ty_mse : 0.00 |\tcommit : 0.05 |\tusage : 0.42 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 0.0001\n",
      "30: \tLoss: 1.332 |\tPSNR: 21.873 |\tMSE: 0.006 |\ty_mse : 0.00 |\tcommit : 0.05 |\tusage : 0.39 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 0.0001\n",
      "31: \tLoss: 1.141 |\tPSNR: 22.618 |\tMSE: 0.005 |\ty_mse : 0.00 |\tcommit : 0.06 |\tusage : 0.48 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 0.0001\n",
      "32: \tLoss: 1.171 |\tPSNR: 22.493 |\tMSE: 0.006 |\ty_mse : 0.00 |\tcommit : 0.06 |\tusage : 0.49 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 0.0001\n",
      "33: \tLoss: 1.188 |\tPSNR: 22.438 |\tMSE: 0.006 |\ty_mse : 0.00 |\tcommit : 0.06 |\tusage : 0.46 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 0.0001\n",
      "34: \tLoss: 1.119 |\tPSNR: 22.700 |\tMSE: 0.005 |\ty_mse : 0.00 |\tcommit : 0.06 |\tusage : 0.45 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 0.0001\n",
      "35: \tLoss: 1.301 |\tPSNR: 22.024 |\tMSE: 0.006 |\ty_mse : 0.00 |\tcommit : 0.06 |\tusage : 0.42 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 0.0001\n",
      "36: \tLoss: 1.182 |\tPSNR: 22.454 |\tMSE: 0.006 |\ty_mse : 0.00 |\tcommit : 0.06 |\tusage : 0.47 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 0.0001\n",
      "37: \tLoss: 1.115 |\tPSNR: 22.742 |\tMSE: 0.005 |\ty_mse : 0.00 |\tcommit : 0.06 |\tusage : 0.50 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 0.0001\n",
      "38: \tLoss: 1.131 |\tPSNR: 22.662 |\tMSE: 0.005 |\ty_mse : 0.00 |\tcommit : 0.06 |\tusage : 0.47 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 0.0001\n",
      "39: \tLoss: 1.206 |\tPSNR: 22.389 |\tMSE: 0.006 |\ty_mse : 0.00 |\tcommit : 0.07 |\tusage : 0.49 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 0.0001\n",
      "40: \tLoss: 1.183 |\tPSNR: 22.471 |\tMSE: 0.006 |\ty_mse : 0.00 |\tcommit : 0.06 |\tusage : 0.49 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 0.0001\n",
      "41: \tLoss: 1.090 |\tPSNR: 22.841 |\tMSE: 0.005 |\ty_mse : 0.00 |\tcommit : 0.06 |\tusage : 0.50 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 0.0001\n",
      "42: \tLoss: 1.091 |\tPSNR: 22.833 |\tMSE: 0.005 |\ty_mse : 0.00 |\tcommit : 0.06 |\tusage : 0.48 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 0.0001\n",
      "43: \tLoss: 1.129 |\tPSNR: 22.683 |\tMSE: 0.005 |\ty_mse : 0.00 |\tcommit : 0.07 |\tusage : 0.51 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 0.0001\n",
      "44: \tLoss: 1.119 |\tPSNR: 22.719 |\tMSE: 0.005 |\ty_mse : 0.00 |\tcommit : 0.07 |\tusage : 0.46 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 0.0001\n",
      "45: \tLoss: 1.133 |\tPSNR: 22.665 |\tMSE: 0.005 |\ty_mse : 0.00 |\tcommit : 0.07 |\tusage : 0.49 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 0.0001\n",
      "46: \tLoss: 1.148 |\tPSNR: 22.622 |\tMSE: 0.005 |\ty_mse : 0.00 |\tcommit : 0.07 |\tusage : 0.49 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 0.0001\n",
      "47: \tLoss: 1.119 |\tPSNR: 22.724 |\tMSE: 0.005 |\ty_mse : 0.00 |\tcommit : 0.07 |\tusage : 0.49 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 0.0001\n",
      "48: \tLoss: 1.106 |\tPSNR: 22.805 |\tMSE: 0.005 |\ty_mse : 0.00 |\tcommit : 0.07 |\tusage : 0.50 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 0.0001\n",
      "49: \tLoss: 1.098 |\tPSNR: 22.828 |\tMSE: 0.005 |\ty_mse : 0.00 |\tcommit : 0.07 |\tusage : 0.46 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 0.0001\n",
      "50: \tLoss: 1.174 |\tPSNR: 22.537 |\tMSE: 0.006 |\ty_mse : 0.00 |\tcommit : 0.07 |\tusage : 0.50 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 0.0001\n",
      "51: \tLoss: 1.191 |\tPSNR: 22.437 |\tMSE: 0.006 |\ty_mse : 0.00 |\tcommit : 0.07 |\tusage : 0.46 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 0.0001\n",
      "52: \tLoss: 1.177 |\tPSNR: 22.499 |\tMSE: 0.006 |\ty_mse : 0.00 |\tcommit : 0.07 |\tusage : 0.45 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 1e-05\n",
      "53: \tLoss: 1.060 |\tPSNR: 22.988 |\tMSE: 0.005 |\ty_mse : 0.00 |\tcommit : 0.07 |\tusage : 0.46 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 1e-05\n",
      "54: \tLoss: 1.049 |\tPSNR: 23.031 |\tMSE: 0.005 |\ty_mse : 0.00 |\tcommit : 0.07 |\tusage : 0.47 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0000 |\n",
      "Learning rate: 1e-05\n",
      "55: \tLoss: 1.050 |\tPSNR: 23.043 |\tMSE: 0.005 |\ty_mse : 0.01 |\tcommit : 0.07 |\tusage : 0.48 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 1e-05\n",
      "56: \tLoss: 1.054 |\tPSNR: 23.020 |\tMSE: 0.005 |\ty_mse : 0.00 |\tcommit : 0.07 |\tusage : 0.50 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 1e-05\n",
      "57: \tLoss: 1.050 |\tPSNR: 23.042 |\tMSE: 0.005 |\ty_mse : 0.01 |\tcommit : 0.07 |\tusage : 0.50 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 1e-05\n",
      "58: \tLoss: 1.068 |\tPSNR: 22.969 |\tMSE: 0.005 |\ty_mse : 0.00 |\tcommit : 0.07 |\tusage : 0.50 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 1e-05\n",
      "59: \tLoss: 1.058 |\tPSNR: 23.008 |\tMSE: 0.005 |\ty_mse : 0.00 |\tcommit : 0.07 |\tusage : 0.49 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 1e-05\n",
      "60: \tLoss: 1.051 |\tPSNR: 23.040 |\tMSE: 0.005 |\ty_mse : 0.00 |\tcommit : 0.07 |\tusage : 0.51 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 1e-05\n",
      "61: \tLoss: 1.057 |\tPSNR: 23.010 |\tMSE: 0.005 |\ty_mse : 0.01 |\tcommit : 0.07 |\tusage : 0.48 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 1e-05\n",
      "62: \tLoss: 1.060 |\tPSNR: 23.001 |\tMSE: 0.005 |\ty_mse : 0.01 |\tcommit : 0.07 |\tusage : 0.49 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 1e-05\n",
      "63: \tLoss: 1.053 |\tPSNR: 23.037 |\tMSE: 0.005 |\ty_mse : 0.01 |\tcommit : 0.07 |\tusage : 0.49 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 1e-05\n",
      "64: \tLoss: 1.041 |\tPSNR: 23.084 |\tMSE: 0.005 |\ty_mse : 0.01 |\tcommit : 0.07 |\tusage : 0.48 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 1e-05\n",
      "65: \tLoss: 1.045 |\tPSNR: 23.073 |\tMSE: 0.005 |\ty_mse : 0.01 |\tcommit : 0.07 |\tusage : 0.50 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 1e-05\n",
      "66: \tLoss: 1.050 |\tPSNR: 23.046 |\tMSE: 0.005 |\ty_mse : 0.01 |\tcommit : 0.07 |\tusage : 0.48 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 1e-05\n",
      "67: \tLoss: 1.060 |\tPSNR: 23.003 |\tMSE: 0.005 |\ty_mse : 0.01 |\tcommit : 0.07 |\tusage : 0.50 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 1e-05\n",
      "68: \tLoss: 1.049 |\tPSNR: 23.056 |\tMSE: 0.005 |\ty_mse : 0.01 |\tcommit : 0.07 |\tusage : 0.49 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 1e-05\n",
      "69: \tLoss: 1.056 |\tPSNR: 23.026 |\tMSE: 0.005 |\ty_mse : 0.01 |\tcommit : 0.07 |\tusage : 0.51 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 1e-05\n",
      "70: \tLoss: 1.037 |\tPSNR: 23.104 |\tMSE: 0.005 |\ty_mse : 0.01 |\tcommit : 0.07 |\tusage : 0.49 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 1e-05\n",
      "71: \tLoss: 1.040 |\tPSNR: 23.099 |\tMSE: 0.005 |\ty_mse : 0.01 |\tcommit : 0.07 |\tusage : 0.51 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 1e-05\n",
      "72: \tLoss: 1.043 |\tPSNR: 23.087 |\tMSE: 0.005 |\ty_mse : 0.01 |\tcommit : 0.07 |\tusage : 0.50 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 1e-05\n",
      "73: \tLoss: 1.024 |\tPSNR: 23.167 |\tMSE: 0.005 |\ty_mse : 0.01 |\tcommit : 0.07 |\tusage : 0.50 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 1e-05\n",
      "74: \tLoss: 1.037 |\tPSNR: 23.111 |\tMSE: 0.005 |\ty_mse : 0.01 |\tcommit : 0.07 |\tusage : 0.50 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 1e-05\n",
      "75: \tLoss: 1.037 |\tPSNR: 23.114 |\tMSE: 0.005 |\ty_mse : 0.01 |\tcommit : 0.07 |\tusage : 0.50 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 1e-05\n",
      "76: \tLoss: 1.033 |\tPSNR: 23.131 |\tMSE: 0.005 |\ty_mse : 0.01 |\tcommit : 0.07 |\tusage : 0.51 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 1e-05\n",
      "77: \tLoss: 1.031 |\tPSNR: 23.140 |\tMSE: 0.005 |\ty_mse : 0.01 |\tcommit : 0.07 |\tusage : 0.50 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 1e-05\n",
      "78: \tLoss: 1.030 |\tPSNR: 23.143 |\tMSE: 0.005 |\ty_mse : 0.01 |\tcommit : 0.07 |\tusage : 0.51 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 1e-05\n",
      "79: \tLoss: 1.034 |\tPSNR: 23.129 |\tMSE: 0.005 |\ty_mse : 0.01 |\tcommit : 0.07 |\tusage : 0.50 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 1e-05\n",
      "80: \tLoss: 1.027 |\tPSNR: 23.155 |\tMSE: 0.005 |\ty_mse : 0.01 |\tcommit : 0.07 |\tusage : 0.49 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 1e-05\n",
      "81: \tLoss: 1.039 |\tPSNR: 23.109 |\tMSE: 0.005 |\ty_mse : 0.01 |\tcommit : 0.07 |\tusage : 0.50 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 1e-05\n",
      "82: \tLoss: 1.029 |\tPSNR: 23.156 |\tMSE: 0.005 |\ty_mse : 0.01 |\tcommit : 0.07 |\tusage : 0.51 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 1e-05\n",
      "83: \tLoss: 1.038 |\tPSNR: 23.114 |\tMSE: 0.005 |\ty_mse : 0.01 |\tcommit : 0.08 |\tusage : 0.50 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 1e-05\n",
      "84: \tLoss: 1.048 |\tPSNR: 23.069 |\tMSE: 0.005 |\ty_mse : 0.01 |\tcommit : 0.07 |\tusage : 0.50 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 1.0000000000000002e-06\n",
      "85: \tLoss: 1.036 |\tPSNR: 23.122 |\tMSE: 0.005 |\ty_mse : 0.01 |\tcommit : 0.07 |\tusage : 0.51 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 1.0000000000000002e-06\n",
      "86: \tLoss: 1.044 |\tPSNR: 23.086 |\tMSE: 0.005 |\ty_mse : 0.01 |\tcommit : 0.07 |\tusage : 0.51 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 1.0000000000000002e-06\n",
      "87: \tLoss: 1.033 |\tPSNR: 23.133 |\tMSE: 0.005 |\ty_mse : 0.01 |\tcommit : 0.07 |\tusage : 0.51 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 1.0000000000000002e-06\n",
      "88: \tLoss: 1.029 |\tPSNR: 23.156 |\tMSE: 0.005 |\ty_mse : 0.01 |\tcommit : 0.07 |\tusage : 0.50 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 1.0000000000000002e-06\n",
      "89: \tLoss: 1.029 |\tPSNR: 23.152 |\tMSE: 0.005 |\ty_mse : 0.01 |\tcommit : 0.07 |\tusage : 0.50 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 1.0000000000000002e-06\n",
      "90: \tLoss: 1.031 |\tPSNR: 23.143 |\tMSE: 0.005 |\ty_mse : 0.01 |\tcommit : 0.07 |\tusage : 0.51 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 1.0000000000000002e-06\n",
      "91: \tLoss: 1.034 |\tPSNR: 23.131 |\tMSE: 0.005 |\ty_mse : 0.01 |\tcommit : 0.07 |\tusage : 0.50 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 1.0000000000000002e-06\n",
      "92: \tLoss: 1.032 |\tPSNR: 23.140 |\tMSE: 0.005 |\ty_mse : 0.01 |\tcommit : 0.07 |\tusage : 0.50 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 1.0000000000000002e-06\n",
      "93: \tLoss: 1.030 |\tPSNR: 23.147 |\tMSE: 0.005 |\ty_mse : 0.01 |\tcommit : 0.08 |\tusage : 0.50 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 1.0000000000000002e-06\n",
      "94: \tLoss: 1.026 |\tPSNR: 23.168 |\tMSE: 0.005 |\ty_mse : 0.01 |\tcommit : 0.08 |\tusage : 0.51 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 1.0000000000000002e-06\n",
      "95: \tLoss: 1.027 |\tPSNR: 23.160 |\tMSE: 0.005 |\ty_mse : 0.01 |\tcommit : 0.08 |\tusage : 0.50 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 1.0000000000000002e-07\n",
      "96: \tLoss: 1.023 |\tPSNR: 23.183 |\tMSE: 0.005 |\ty_mse : 0.01 |\tcommit : 0.08 |\tusage : 0.50 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 1.0000000000000002e-07\n",
      "97: \tLoss: 1.024 |\tPSNR: 23.173 |\tMSE: 0.005 |\ty_mse : 0.01 |\tcommit : 0.08 |\tusage : 0.50 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 1.0000000000000002e-07\n",
      "98: \tLoss: 1.030 |\tPSNR: 23.149 |\tMSE: 0.005 |\ty_mse : 0.01 |\tcommit : 0.08 |\tusage : 0.49 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 1.0000000000000002e-07\n",
      "99: \tLoss: 1.037 |\tPSNR: 23.125 |\tMSE: 0.005 |\ty_mse : 0.01 |\tcommit : 0.08 |\tusage : 0.49 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 1.0000000000000002e-07\n",
      "100: \tLoss: 1.041 |\tPSNR: 23.095 |\tMSE: 0.005 |\ty_mse : 0.01 |\tcommit : 0.07 |\tusage : 0.49 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 1.0000000000000002e-07\n",
      "101: \tLoss: 1.040 |\tPSNR: 23.101 |\tMSE: 0.005 |\ty_mse : 0.01 |\tcommit : 0.07 |\tusage : 0.49 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 1.0000000000000002e-07\n",
      "102: \tLoss: 1.033 |\tPSNR: 23.132 |\tMSE: 0.005 |\ty_mse : 0.01 |\tcommit : 0.07 |\tusage : 0.49 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 1.0000000000000002e-07\n",
      "103: \tLoss: 1.032 |\tPSNR: 23.136 |\tMSE: 0.005 |\ty_mse : 0.01 |\tcommit : 0.08 |\tusage : 0.50 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 1.0000000000000002e-07\n",
      "104: \tLoss: 1.040 |\tPSNR: 23.108 |\tMSE: 0.005 |\ty_mse : 0.01 |\tcommit : 0.08 |\tusage : 0.50 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 1.0000000000000002e-07\n",
      "105: \tLoss: 1.033 |\tPSNR: 23.136 |\tMSE: 0.005 |\ty_mse : 0.01 |\tcommit : 0.08 |\tusage : 0.50 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 1.0000000000000002e-07\n",
      "106: \tLoss: 1.029 |\tPSNR: 23.154 |\tMSE: 0.005 |\ty_mse : 0.01 |\tcommit : 0.08 |\tusage : 0.50 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n",
      "Learning rate: 1.0000000000000002e-07\n",
      "107: \tLoss: 1.024 |\tPSNR: 23.176 |\tMSE: 0.005 |\ty_mse : 0.01 |\tcommit : 0.08 |\tusage : 0.49 |\tCEL : 2.05 |\tBpp: 0.04 |\tH_mse: 0.0001 |\n"
     ]
    }
   ],
   "source": [
    "# (self, N=128, quantizers=1, CB_size=512, dim=64, **kwargs):\n",
    "net = Hyper_VQ(128, quantizers=quantizers, CB_size=CB_size_setting, dim=dim).to(device)\n",
    "optimizer, hyper_optimizer, aux_optimizer = configure_optimizers(net)\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"min\")\n",
    "criterion = RateDistortionLoss(lmbda=lambda_setting)\n",
    "\n",
    "path = 'vq.tar'\n",
    "last_epoch = 0\n",
    "if False:\n",
    "    print(\"Loading\", path)\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    last_epoch = checkpoint[\"epoch\"] + 1\n",
    "    net.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "    aux_optimizer.load_state_dict(checkpoint[\"aux_optimizer\"])\n",
    "    lr_scheduler.load_state_dict(checkpoint[\"lr_scheduler\"])\n",
    "\n",
    "with open(f'{log_path}.csv','a') as f:\n",
    "    log_s = f\"epochs, PSNR, MSE, y_mse, commit, usage, CE, bpp, hyper_loss\\n\"\n",
    "    f.write(log_s)\n",
    "\n",
    "best_loss = float(\"inf\")\n",
    "for epoch in range(last_epoch, epochs):\n",
    "    lring = optimizer.param_groups[0]['lr']\n",
    "    if lring < 1e-7:\n",
    "        break\n",
    "    print(f\"Learning rate: {lring}\")\n",
    "    train_one_epoch(\n",
    "        net,\n",
    "        criterion,\n",
    "        train_dataloader,\n",
    "        optimizer,\n",
    "        hyper_optimizer,\n",
    "        aux_optimizer,\n",
    "        epoch,\n",
    "        1,\n",
    "    )\n",
    "    loss = test_epoch(epoch, test_dataloader, net, criterion)\n",
    "    lr_scheduler.step(loss)\n",
    "\n",
    "    is_best = loss < best_loss\n",
    "    best_loss = min(loss, best_loss)\n",
    "\n",
    "    save_checkpoint(\n",
    "        {\n",
    "            \"epoch\": epoch,\n",
    "            \"state_dict\": net.state_dict(),\n",
    "            \"loss\": loss,\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"aux_optimizer\": aux_optimizer.state_dict(),\n",
    "            \"lr_scheduler\": lr_scheduler.state_dict(),\n",
    "        },\n",
    "        is_best,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.889640960123728, 0.8796447515487671, 0.042132019996643066\n",
      "\n"
     ]
    }
   ],
   "source": [
    "psnr, msssim, zbpp = eval_image(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.17648338550712\n",
      "tensor(5.2069e-05, device='cuda:0')\n",
      "tensor(0.1243, device='cuda:0') tensor(0.1108, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model = net\n",
    "with torch.no_grad():\n",
    "    for d in test_dataloader:\n",
    "        d = d.to(device)\n",
    "\n",
    "        y = model.g_a(d)\n",
    "        if hyper_enable and not param_direct:\n",
    "            y_std, ce, z_likelihoods, scales_hat, means_hat = model.represent_befor_quantize(y)    \n",
    "        elif hyper_enable and param_direct:\n",
    "            y_std, ce, z_likelihoods, scales_hat, means_hat, gp, gp_hat = model.direct_calc_gaussian(y)\n",
    "        else:\n",
    "            y_std = y\n",
    "            ce = model.calc_cross_entropy(y)\n",
    "            z_likelihoods = torch.tensor(0)\n",
    "\n",
    "        y_hat, idd, commit, usage = model.vq(y_std)  # (b, Q, w, h), (b, Q, w, h), (b), (b)\n",
    "        y_hat_ = model.destandardized(y_hat, scales_hat, means_hat) if hyper_enable else y_hat\n",
    "        x_hat = model.g_s(y_hat_)\n",
    "\n",
    "x_hat_ =  x_hat.clamp(0, 1)\n",
    "print(compute_psnr(x_hat_, d))        \n",
    "b, c, h, w = d.shape\n",
    "print(F.mse_loss(gp_hat, gp))\n",
    "print(gp_hat[0, 0, 0 ,1], gp[0, 0, 0 ,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_and_sigma(symbol):\n",
    "    symbol_flat = symbol.view(-1)    \n",
    "    mean = symbol_flat.mean()\n",
    "    mean_flat = torch.zeros_like(symbol_flat) + mean    \n",
    "    sigma = ((symbol_flat - mean_flat) ** 2).mean()\n",
    "    return mean, sigma\n",
    "m, s = get_mean_and_sigma(y)\n",
    "m_, s_ = get_mean_and_sigma(y_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100, 634\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def draw_distribution(data, data2):\n",
    "    diff = data.max() - data.min()\n",
    "    diff2 = data2.max() - data2.min()    \n",
    "\n",
    "    if diff > diff2:\n",
    "        flag = True\n",
    "        d1 = data2\n",
    "        d2 = data\n",
    "    else:\n",
    "        flag = False\n",
    "        d1 = data\n",
    "        d2 = data2\n",
    "\n",
    "    s1 = d1.min()\n",
    "    e1 = d1.max()\n",
    "    s2 = d2.min()\n",
    "    e2 = d2.max()\n",
    "\n",
    "    y = torch.histc(d1)    \n",
    "    bins = len(y)\n",
    "    x = torch.linspace(s1, e1, bins)\n",
    "    \n",
    "    bins2 = (bins * (e2 - s2) / (e1 - s1)).round().int()\n",
    "    y2 = torch.histc(d2, bins=bins2)\n",
    "    x2 = torch.linspace(s2, e2, bins2)\n",
    "\n",
    "    if flag:\n",
    "        plt.plot(x2.cpu().detach().numpy(), y2.cpu().detach().numpy())        \n",
    "        plt.plot(x.cpu().detach().numpy(), y.cpu().detach().numpy())\n",
    "    else:\n",
    "        plt.plot(x.cpu().detach().numpy(), y.cpu().detach().numpy())\n",
    "        plt.plot(x2.cpu().detach().numpy(), y2.cpu().detach().numpy())\n",
    "    print(f'{bins}, {bins2}')\n",
    "    \n",
    "draw_distribution(y, y_std)\n",
    "plt.savefig('temp.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100, 105\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ll = len(y_std.view(-1))\n",
    "n01 = torch.randn(ll).to(device)\n",
    "\n",
    "draw_distribution(n01, y_std)\n",
    "plt.savefig('temp.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_CE(symbol):\n",
    "    CE = net.calc_cross_entropy(symbol)\n",
    "    CE_avg = torch.log(CE).sum() / (-math.log(2) * (b * h * w))\n",
    "    return CE_avg.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",mse wt, 0.003\n",
      ",yCE, 1.4040321111679077\n",
      ",y_CE, 2.048673391342163\n",
      ",ymu, 0.11357956379652023\n",
      ",ysigma, 0.015996074303984642\n",
      ",y_mu, 5.518948631433318e-10\n",
      ",y_sigma, 0.9988143444061279\n",
      ",PSNR, 23.889640960123728\n",
      ",ms-ssim, 0.8796447515487671\n",
      ",usage, 0.49295762181282043\n",
      ",zbpp, 0.042132019996643066\n",
      ",h_mse, 5.206936475588009e-05\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_s = f\",mse wt, {lambda_setting}\\n\"\n",
    "log_s = log_s + f\",yCE, {get_CE(y)}\\n\"\n",
    "log_s = log_s + f\",y_CE, {get_CE(y_std)}\\n\"\n",
    "log_s = log_s + f\",ymu, {m}\\n\"\n",
    "log_s = log_s + f\",ysigma, {s}\\n\"\n",
    "log_s = log_s + f\",y_mu, {m_}\\n\"\n",
    "log_s = log_s + f\",y_sigma, {s_}\\n\"\n",
    "log_s = log_s + f\",PSNR, {psnr}\\n\"\n",
    "log_s = log_s + f\",ms-ssim, {msssim}\\n\"\n",
    "log_s = log_s + f\",usage, {usage.item()}\\n\"\n",
    "log_s = log_s + f\",zbpp, {zbpp}\\n\"\n",
    "log_s = log_s + f\",h_mse, {F.mse_loss(gp, gp_hat)}\\n\"\n",
    "print(log_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{log_path}.csv','a') as f:\n",
    "    f.write(log_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = torch.randn(18, 64, 128, 128).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def standardized(y, means, scales):\n",
    "#     variance = model.lower_bound_s(scales)\n",
    "#     y_std = y - means    \n",
    "#     y_std = y_std / variance\n",
    "#     ce_N01 = model.calc_cross_entropy(y_std)\n",
    "#     return y_std, ce_N01\n",
    "\n",
    "\n",
    "# def direct_calc_gaussian_(y):\n",
    "#     means = y.mean(1).unsqueeze(1)\n",
    "#     scales = ((y - means) ** 2).mean(1).unsqueeze(1)\n",
    "#     variance = torch.sqrt(scales)    \n",
    "#     gaussian_params = torch.cat([variance, means], 1)    \n",
    "#     z = model.h_a(gaussian_params)\n",
    "#     z_hat, z_likelihoods = model.entropy_bottleneck(z)\n",
    "#     gp_hat = model.h_s(z_hat)    \n",
    "#     scales_hat, means_hat = gp_hat.chunk(2, 1)    \n",
    "# #     print(F.mse_loss(gaussian_params, gp_hat))    \n",
    "#     y_std, ce = standardized(y, means_hat, scales_hat)\n",
    "    \n",
    "#     print(f'{scales_hat[0, 0, 0, 0]}, {variance[0, 0, 0, 0]}')\n",
    "#     print(F.mse_loss(scales_hat, variance))\n",
    "\n",
    "#     print(f'{means_hat[0, 0, 0, 0]}, {means[0, 0, 0, 0]}')\n",
    "#     print(F.mse_loss(means_hat, means))\n",
    "\n",
    "#     print(F.mse_loss(gaussian_params, gp_hat))    \n",
    "    \n",
    "#     return y_std\n",
    "\n",
    "# xx = torch.randn(18, 64, 128, 128).to(device)\n",
    "# xxx = direct_calc_gaussian_(xx)\n",
    "# print(xx[0, :, 0, 0])\n",
    "# print(xxx[0, :, 0, 0])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "4b3695a85a4caa0373e4268d79926f2001c5518f41e76a28112350d8a4ba6e82"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
