{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1042, 0.0842, 0.1909, 0.2406], grad_fn=<CatBackward>)\n",
      "tensor([0.1250, 0.0964, 0.0625, 0.0625])\n",
      "tensor(0.0866)\n"
     ]
    }
   ],
   "source": [
    "from vector_quantize_pytorch.vector_quantize_pytorch import VectorQuantize\n",
    "from vector_quantize_pytorch.residual_vq import ResidualVQ, MultiLayerVQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(12.)\n"
     ]
    }
   ],
   "source": [
    "from compressai.models.utils import conv, deconv, update_registered_buffers\n",
    "from compressai.layers import GDN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import warnings\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from compressai.models import ScaleHyperprior\n",
    "from compressai.entropy_models import EntropyBottleneck, GaussianConditional\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "from torch import Tensor\n",
    "\n",
    "from typing import Any, Callable, List, Optional, Tuple, Union\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from compressai.datasets import ImageFolder\n",
    "import torch.optim as optim\n",
    "import shutil\n",
    "from pytorch_msssim import ms_ssim\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "from compressai.layers import (\n",
    "    AttentionBlock,\n",
    "    ResidualBlock,\n",
    "    ResidualBlockUpsample,\n",
    "    ResidualBlockWithStride,\n",
    "    conv3x3,\n",
    "    subpel_conv3x3,\n",
    ")\n",
    "from compressai.ops import LowerBound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_enable = True\n",
    "ce_loss_enable = True\n",
    "log_path = \"log\"\n",
    "version = \"test\"\n",
    "lambda_setting = 0.003\n",
    "quantizers = 2\n",
    "CB_size_setting = 256\n",
    "dim = 64\n",
    "workers_setting = 4\n",
    "batch_size_setting = 8\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hyper_VQ(ScaleHyperprior):\n",
    "    def __init__(self, N=128, quantizers=1, CB_size=512, dim=64, **kwargs):\n",
    "        super().__init__(N, N, **kwargs)\n",
    "        self.gaussian_conditional = GaussianConditional(None)\n",
    "\n",
    "        self.lower_bound_l = LowerBound(1e-9)\n",
    "        self.lower_bound_s = LowerBound(0.11)\n",
    "\n",
    "        self.g_a = nn.Sequential(\n",
    "            ResidualBlockWithStride(3, N, stride=2),\n",
    "            AttentionBlock(N),\n",
    "            ResidualBlock(N, N),\n",
    "            ResidualBlockWithStride(N, N, stride=2),\n",
    "            AttentionBlock(N),\n",
    "            ResidualBlock(N, N),\n",
    "            conv3x3(N, dim, stride=2),\n",
    "            AttentionBlock(dim),\n",
    "            ResidualBlock(dim, dim),\n",
    "        )\n",
    "        self.h_a = nn.Sequential(\n",
    "            conv3x3(dim, N),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            conv3x3(N, N),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            conv3x3(N, N, stride=2),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            conv3x3(N, N),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            conv3x3(N, N, stride=2),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            conv3x3(N, N),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            conv3x3(N, N),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            conv3x3(N, N, stride=2),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            conv3x3(N, N),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            conv3x3(N, N),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            conv3x3(N, N, stride=2),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            conv3x3(N, N),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            conv3x3(N, N, stride=2),\n",
    "            \n",
    "        )\n",
    "        self.h_s = nn.Sequential(\n",
    "            conv3x3(N, N),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            subpel_conv3x3(N, N, 2),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            conv3x3(N, N),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            subpel_conv3x3(N, N, 2),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            conv3x3(N, N),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            subpel_conv3x3(N, N, 2),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            conv3x3(N, N),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            subpel_conv3x3(N, N, 2),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            conv3x3(N, N),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            subpel_conv3x3(N, N, 2),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            conv3x3(N, dim * 2),\n",
    "#             conv3x3(N, 2),\n",
    "        )\n",
    "        self.g_s = nn.Sequential(\n",
    "            AttentionBlock(dim),\n",
    "            ResidualBlock(dim, dim),\n",
    "            ResidualBlockUpsample(dim, N, 2),\n",
    "            AttentionBlock(N),\n",
    "            ResidualBlock(N, N),\n",
    "            ResidualBlockUpsample(N, N, 2),\n",
    "            AttentionBlock(N),\n",
    "            ResidualBlock(N, N),\n",
    "            subpel_conv3x3(N, 3, 2),\n",
    "        )\n",
    "        if quantizers == 1:\n",
    "            self.vq = VectorQuantize(\n",
    "                dim = dim,\n",
    "                codebook_size = CB_size,    # codebook size\n",
    "                decay = 0.99,               # the exponential moving average decay, lower means the dictionary will change faster\n",
    "                commitment_weight = 0.25,   # the weight on the commitment loss\n",
    "                accept_image_fmap = True,\n",
    "            )\n",
    "        elif quantizers > 1:\n",
    "            self.vq = MultiLayerVQ(\n",
    "                num_quantizers = quantizers,\n",
    "                dim = dim // quantizers,\n",
    "                codebook_size = CB_size,    # codebook size\n",
    "                decay = 0.99,               # the exponential moving average decay, lower means the dictionary will change faster\n",
    "                commitment_weight = 0.25,   # the weight on the commitment loss\n",
    "                accept_image_fmap = True,\n",
    "            )\n",
    "    def calc_cross_entropy(self, symbol):\n",
    "        ones = torch.ones_like(symbol).to(device).float()\n",
    "        cross_entropy_from_N_01 = self.gaussian_conditional._likelihood(symbol, ones)\n",
    "        cross_entropy_from_N_01 = self.lower_bound_l(cross_entropy_from_N_01)\n",
    "        return cross_entropy_from_N_01\n",
    "\n",
    "    def standardized(self, y, means, scales):\n",
    "        variance = self.lower_bound_s(scales)\n",
    "        y_std = y - means\n",
    "        y_std = y_std / variance\n",
    "        ce_N01 = self.calc_cross_entropy(y_std)\n",
    "        return y_std, ce_N01\n",
    "        \n",
    "    def destandardized(self, y_std, means, scales):\n",
    "        variance = self.lower_bound_s(scales)\n",
    "        y_ = y_std * variance\n",
    "        y_ = y_ + means\n",
    "        return y_\n",
    "\n",
    "    def represent_befor_quantize(self, y):\n",
    "        z = self.h_a(y)\n",
    "        z_hat, z_likelihoods = self.entropy_bottleneck(z)\n",
    "        gaussian_params = self.h_s(z_hat)\n",
    "        scales_hat, means_hat = gaussian_params.chunk(2, 1)\n",
    "        y_std, ce = self.standardized(y, means_hat, scales_hat)\n",
    "        return y_std, ce, z_likelihoods, scales_hat, means_hat\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.g_a(x)\n",
    "        if hyper_enable:\n",
    "            y_std, ce, z_likelihoods, scales_hat, means_hat = self.represent_befor_quantize(y)\n",
    "        else:\n",
    "            y_std = y\n",
    "            ce = self.calc_cross_entropy(y)\n",
    "            z_likelihoods = torch.tensor(0)\n",
    "        \n",
    "        y_hat, id, commit, usage = self.vq(y_std)  # (b, Q, w, h), (b, Q, w, h), (b), (b)\n",
    "        y_hat_ = self.destandardized(y_hat, scales_hat, means_hat) if hyper_enable else y_hat\n",
    "        x_hat = self.g_s(y_hat_)\n",
    "\n",
    "        return {\n",
    "            \"x_hat\": x_hat,\n",
    "            \"likelihoods\": {\"z\": z_likelihoods},\n",
    "            \"commit\": commit,\n",
    "            \"usage\": usage,\n",
    "            \"cross\": ce,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_psnr(a, b):\n",
    "    mse = torch.mean((a - b)**2).item()\n",
    "    return -10 * math.log10(mse)\n",
    "\n",
    "def compute_mse(a, b):\n",
    "    mse = torch.mean((a - b)**2).item()\n",
    "    return mse\n",
    "\n",
    "def compute_msssim(a, b):\n",
    "    return ms_ssim(a, b, data_range=1.).item()\n",
    "\n",
    "def compute_bpp(out_net):\n",
    "    size = out_net['x_hat'].size()\n",
    "    num_pixels = size[0] * size[2] * size[3]\n",
    "    return sum(torch.log(likelihoods).sum() / (-math.log(2) * num_pixels)\n",
    "              for likelihoods in out_net['likelihoods'].values()).item()\n",
    "\n",
    "# def configure_optimizers(net):\n",
    "#     \"\"\"Separate parameters for the main optimizer and the auxiliary optimizer.\n",
    "#     Return two optimizers\"\"\"\n",
    "\n",
    "#     parameters = {\n",
    "#         n\n",
    "#         for n, p in net.named_parameters()\n",
    "#         if not n.endswith(\".quantiles\") and p.requires_grad\n",
    "#     }\n",
    "#     aux_parameters = {\n",
    "#         n\n",
    "#         for n, p in net.named_parameters()\n",
    "#         if n.endswith(\".quantiles\") and p.requires_grad\n",
    "#     }\n",
    "\n",
    "#     # Make sure we don't have an intersection of parameters\n",
    "#     params_dict = dict(net.named_parameters())\n",
    "#     inter_params = parameters & aux_parameters\n",
    "#     union_params = parameters | aux_parameters\n",
    "\n",
    "#     assert len(inter_params) == 0\n",
    "#     assert len(union_params) - len(params_dict.keys()) == 0\n",
    "\n",
    "#     optimizer = optim.Adam(\n",
    "#         (params_dict[n] for n in sorted(parameters)),\n",
    "#         lr=1e-4,\n",
    "#     )\n",
    "#     aux_optimizer = optim.Adam(\n",
    "#         (params_dict[n] for n in sorted(aux_parameters)),\n",
    "#         lr=1e-3,\n",
    "#     )\n",
    "#     return optimizer, aux_optimizer\n",
    "\n",
    "class AverageMeter:\n",
    "    \"\"\"Compute running average.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        \n",
    "def configure_optimizers(net):\n",
    "    \"\"\"Separate parameters for the main optimizer and the auxiliary optimizer.\n",
    "    Return two optimizers\"\"\"\n",
    "    parameters = {\n",
    "        n\n",
    "        for n, p in net.named_parameters()\n",
    "        if not n.endswith(\".quantiles\") and p.requires_grad and not n.startswith(\"h\")\n",
    "    }\n",
    "    aux_parameters = {\n",
    "        n\n",
    "        for n, p in net.named_parameters()\n",
    "        if n.endswith(\".quantiles\") and p.requires_grad\n",
    "    }\n",
    "    hyper_parameters = {\n",
    "            n\n",
    "        for n, p in net.named_parameters()\n",
    "        if n.startswith(\"h\") and p.requires_grad\n",
    "    }\n",
    "\n",
    "    # Make sure we don't have an intersection of parameters\n",
    "    params_dict = dict(net.named_parameters())\n",
    "    inter_params = parameters & aux_parameters & hyper_parameters\n",
    "    union_params = parameters | aux_parameters | hyper_parameters\n",
    "\n",
    "    assert len(inter_params) == 0\n",
    "    assert len(union_params) - len(params_dict.keys()) == 0\n",
    "\n",
    "    optimizer = optim.Adam(\n",
    "        (params_dict[n] for n in sorted(parameters)),\n",
    "        lr=1e-4,\n",
    "    )\n",
    "    hyper_optimizer = optim.Adam(\n",
    "        (params_dict[n] for n in sorted(hyper_parameters)),\n",
    "        lr=1e-4,\n",
    "    )\n",
    "    aux_optimizer = optim.Adam(\n",
    "        (params_dict[n] for n in sorted(aux_parameters)),\n",
    "        lr=1e-3,\n",
    "    )\n",
    "    return optimizer, hyper_optimizer, aux_optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function, Train, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "class RateDistortionLoss(nn.Module):\n",
    "    \"\"\"Custom rate distortion loss with a Lagrangian parameter.\"\"\"\n",
    "\n",
    "    def __init__(self, lmbda=1e-2):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.lmbda = lmbda\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        N, _, H, W = target.size()\n",
    "        out = {}\n",
    "        num_pixels = N * H * W\n",
    "        out[\"mse_loss\"] = self.mse(output[\"x_hat\"], target)\n",
    "        out[\"commit\"] = output[\"commit\"]\n",
    "        out[\"perplexity\"] = output[\"usage\"]\n",
    "        if hyper_enable:        \n",
    "            out[\"bpp_loss\"] = sum(\n",
    "                (torch.log(likelihoods).sum() / (-math.log(2) * num_pixels))\n",
    "                for likelihoods in output[\"likelihoods\"].values()\n",
    "            )\n",
    "        else:\n",
    "            out[\"bpp_loss\"] = 0\n",
    "        out[\"ce_loss\"] = torch.log(output[\"cross\"]).sum() / (-math.log(2) * num_pixels)\n",
    "        if ce_loss_enable:\n",
    "            out[\"loss\"] = self.lmbda * 255**2 *  out[\"mse_loss\"] + out[\"commit\"].sum() + out[\"ce_loss\"]\n",
    "        else:\n",
    "            out[\"loss\"] = self.lmbda * 255**2 *  out[\"mse_loss\"] + out[\"commit\"].sum()\n",
    "        return out\n",
    "\n",
    "def train_one_epoch(\n",
    "    model, criterion, train_dataloader, optimizer, hyper_optimizer, aux_optimizer, epoch, clip_max_norm\n",
    "):\n",
    "    model.train()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    for i, d in enumerate(train_dataloader):\n",
    "        d = d.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        hyper_optimizer.zero_grad()\n",
    "        aux_optimizer.zero_grad()\n",
    "        out_net = model(d)\n",
    "        \n",
    "        out_criterion = criterion(out_net, d)\n",
    "        out_criterion[\"loss\"].backward()\n",
    "        optimizer.step()\n",
    "        if hyper_enable:\n",
    "            hyper_optimizer.step()\n",
    "        aux_loss = model.aux_loss()\n",
    "        aux_loss.backward()\n",
    "        aux_optimizer.step()\n",
    "\n",
    "        if i == 100:\n",
    "            print(\n",
    "                f\"Train epoch {epoch}: [\"\n",
    "                f\"{i*len(d)}/{len(train_dataloader.dataset)}\"\n",
    "                f\" ({100. * i / len(train_dataloader):.0f}%)]\"\n",
    "                f'\\t Loss: {out_criterion[\"loss\"].item():.3f} |'\n",
    "                f'\\t MSE loss: {out_criterion[\"mse_loss\"].item():.3f} |'\n",
    "                f'\\t Bpp loss: {out_criterion[\"bpp_loss\"].item():.2f} |'                \n",
    "                f'\\t vq commit:{out_criterion[\"commit\"].sum().item():.2f} |'\n",
    "                f'\\t perplexity:{out_criterion[\"perplexity\"].mean().item():.2f} |'\n",
    "                f\"\\t Aux loss: {aux_loss.item():.2f}\"\n",
    "            )\n",
    "            \n",
    "def test_epoch(epoch, test_dataloader, model:Hyper_VQ, criterion):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    loss = AverageMeter()\n",
    "    psnr_score = AverageMeter()\n",
    "    mse_loss = AverageMeter()\n",
    "    bpp_loss = AverageMeter()\n",
    "    aux_loss = AverageMeter()\n",
    "    y_mse = AverageMeter()\n",
    "    commit_loss = AverageMeter()\n",
    "    usage_status = AverageMeter()\n",
    "    CE_loss = AverageMeter()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for d in test_dataloader:\n",
    "            d = d.to(device)\n",
    "\n",
    "            y = model.g_a(d)\n",
    "            if hyper_enable:\n",
    "                y_std, ce, z_likelihoods, scales_hat, means_hat = model.represent_befor_quantize(y)\n",
    "            else:\n",
    "                y_std = y\n",
    "                ce = model.calc_cross_entropy(y)\n",
    "                z_likelihoods = torch.tensor(0)\n",
    "            \n",
    "            y_hat, id, commit, usage = model.vq(y_std)  # (b, Q, w, h), (b, Q, w, h), (b), (b)\n",
    "            y_hat_ = model.destandardized(y_hat, scales_hat, means_hat) if hyper_enable else y_hat\n",
    "            x_hat = model.g_s(y_hat_)\n",
    "\n",
    "            out_net = {\n",
    "                \"x_hat\": x_hat,\n",
    "                \"likelihoods\": {\"z\": z_likelihoods},\n",
    "                \"commit\": commit,\n",
    "                \"usage\": usage,\n",
    "                \"cross\": ce,\n",
    "            }\n",
    "\n",
    "            out_criterion = criterion(out_net, d)\n",
    "            loss.update(out_criterion[\"loss\"])\n",
    "            psnr_score.update(compute_psnr(x_hat, d))\n",
    "            mse_loss.update(out_criterion[\"mse_loss\"])\n",
    "            bpp_loss.update(out_criterion[\"bpp_loss\"])\n",
    "            aux_loss.update(model.aux_loss())\n",
    "            y_criterion = compute_mse(y_hat_, y)\n",
    "            y_mse.update(y_criterion)\n",
    "            commit_loss.update(out_criterion[\"commit\"].sum())\n",
    "            usage_status.update(out_criterion[\"perplexity\"])\n",
    "            CE_loss.update(out_criterion[\"ce_loss\"])\n",
    "                \n",
    "    print(\n",
    "        f\"{epoch}: \"\n",
    "        f\"\\tLoss: {loss.avg:.3f} |\"\n",
    "        f\"\\tPSNR: {psnr_score.avg:.3f} |\"\n",
    "        f\"\\tMSE: {mse_loss.avg:.3f} |\"\n",
    "        f\"\\ty_mse : {y_mse.avg:.2f} |\"\n",
    "        f\"\\tcommit : {commit_loss.avg:.2f} |\"\n",
    "        f\"\\tusage : {usage_status.avg:.2f} |\"\n",
    "        f\"\\tCEL : {CE_loss.avg:.2f} |\"\n",
    "        f\"\\tBpp: {bpp_loss.avg:.2f} |\"\n",
    "    )\n",
    "    with open(f'{log_path}.csv','a') as f:\n",
    "        log_s = f\"{epoch}, {psnr_score.avg}, {mse_loss.avg}, {y_mse.avg}, {commit_loss.avg}, {usage_status.avg}, {CE_loss.avg}, {bpp_loss.avg}\\n\"\n",
    "        f.write(log_s)\n",
    "    return loss.avg\n",
    "\n",
    "def eval_image(net):\n",
    "    device = next(net.parameters()).device\n",
    "    img = Image.open('../assets/kodim15.png').convert('RGB')\n",
    "    x = transforms.ToTensor()(img).unsqueeze(0).to(device = device)\n",
    "    with torch.no_grad():\n",
    "        out_net = net.forward(x)\n",
    "    out_net['x_hat'].clamp_(0, 1)\n",
    "    psnr = compute_psnr(x, out_net[\"x_hat\"])\n",
    "    msssim = compute_msssim(x, out_net[\"x_hat\"])\n",
    "    if 'likelihoods' in out_net:\n",
    "        bitrate = compute_bpp(out_net)\n",
    "    else:\n",
    "        bitrate = 0\n",
    "    log_s = f\"{psnr}, {msssim}, {bitrate}\\n\"\n",
    "    print(log_s)\n",
    "    torchvision.utils.save_image(out_net['x_hat'], f'{version}.png')\n",
    "    return psnr, msssim, bitrate\n",
    "\n",
    "def save_checkpoint(state, is_best, filename=f'{version}.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, \"best.vq.tar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose(\n",
    "    [transforms.RandomCrop((256, 256)), transforms.ToTensor()]\n",
    ")\n",
    "\n",
    "test_transforms = transforms.Compose(\n",
    "    [transforms.ToTensor()]\n",
    ")\n",
    "\n",
    "train_dataset = ImageFolder('../../../DIV2K_HR', split=\"train\", transform=train_transforms)\n",
    "test_dataset = ImageFolder('../../../DIV2K_HR', split=\"test\", transform=test_transforms)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size = batch_size_setting,\n",
    "    num_workers = workers_setting,\n",
    "    shuffle=True,\n",
    "    pin_memory=(device == \"cuda\"),\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size = 18,\n",
    "    num_workers = workers_setting,\n",
    "    shuffle=False,\n",
    "    pin_memory=(device == \"cuda\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.0001\n",
      "0: \tLoss: 3.631 |\tPSNR: 17.744 |\tMSE: 0.017 |\ty_mse : 0.04 |\tcommit : 0.35 |\tusage : 0.01 |\tCEL : 3.81 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "1: \tLoss: 2.896 |\tPSNR: 18.980 |\tMSE: 0.013 |\ty_mse : 0.05 |\tcommit : 0.43 |\tusage : 0.01 |\tCEL : 5.28 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "2: \tLoss: 2.588 |\tPSNR: 19.577 |\tMSE: 0.011 |\ty_mse : 0.03 |\tcommit : 0.44 |\tusage : 0.01 |\tCEL : 5.33 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "3: \tLoss: 2.354 |\tPSNR: 19.769 |\tMSE: 0.011 |\ty_mse : 0.03 |\tcommit : 0.30 |\tusage : 0.02 |\tCEL : 4.80 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "4: \tLoss: 2.385 |\tPSNR: 19.821 |\tMSE: 0.010 |\ty_mse : 0.03 |\tcommit : 0.35 |\tusage : 0.02 |\tCEL : 5.39 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "5: \tLoss: 2.215 |\tPSNR: 20.148 |\tMSE: 0.010 |\ty_mse : 0.02 |\tcommit : 0.33 |\tusage : 0.02 |\tCEL : 5.58 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "6: \tLoss: 2.223 |\tPSNR: 20.232 |\tMSE: 0.009 |\ty_mse : 0.02 |\tcommit : 0.37 |\tusage : 0.02 |\tCEL : 5.48 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "7: \tLoss: 2.253 |\tPSNR: 20.046 |\tMSE: 0.010 |\ty_mse : 0.02 |\tcommit : 0.32 |\tusage : 0.02 |\tCEL : 5.34 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "8: \tLoss: 2.070 |\tPSNR: 20.433 |\tMSE: 0.009 |\ty_mse : 0.02 |\tcommit : 0.30 |\tusage : 0.02 |\tCEL : 4.94 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "9: \tLoss: 1.889 |\tPSNR: 20.729 |\tMSE: 0.008 |\ty_mse : 0.02 |\tcommit : 0.24 |\tusage : 0.02 |\tCEL : 4.44 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "10: \tLoss: 1.568 |\tPSNR: 21.383 |\tMSE: 0.007 |\ty_mse : 0.02 |\tcommit : 0.15 |\tusage : 0.03 |\tCEL : 4.50 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "11: \tLoss: 1.443 |\tPSNR: 21.841 |\tMSE: 0.007 |\ty_mse : 0.02 |\tcommit : 0.17 |\tusage : 0.06 |\tCEL : 4.44 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "12: \tLoss: 1.285 |\tPSNR: 22.247 |\tMSE: 0.006 |\ty_mse : 0.02 |\tcommit : 0.12 |\tusage : 0.09 |\tCEL : 4.06 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "13: \tLoss: 1.277 |\tPSNR: 22.342 |\tMSE: 0.006 |\ty_mse : 0.02 |\tcommit : 0.14 |\tusage : 0.12 |\tCEL : 4.12 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "14: \tLoss: 1.180 |\tPSNR: 22.618 |\tMSE: 0.005 |\ty_mse : 0.02 |\tcommit : 0.11 |\tusage : 0.13 |\tCEL : 3.85 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "15: \tLoss: 1.225 |\tPSNR: 22.488 |\tMSE: 0.006 |\ty_mse : 0.02 |\tcommit : 0.13 |\tusage : 0.14 |\tCEL : 3.89 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "16: \tLoss: 1.221 |\tPSNR: 22.468 |\tMSE: 0.006 |\ty_mse : 0.02 |\tcommit : 0.12 |\tusage : 0.16 |\tCEL : 3.72 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "17: \tLoss: 1.103 |\tPSNR: 22.978 |\tMSE: 0.005 |\ty_mse : 0.02 |\tcommit : 0.12 |\tusage : 0.16 |\tCEL : 3.82 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "18: \tLoss: 1.074 |\tPSNR: 23.085 |\tMSE: 0.005 |\ty_mse : 0.02 |\tcommit : 0.12 |\tusage : 0.17 |\tCEL : 3.77 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "19: \tLoss: 1.051 |\tPSNR: 23.212 |\tMSE: 0.005 |\ty_mse : 0.02 |\tcommit : 0.12 |\tusage : 0.17 |\tCEL : 3.80 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "20: \tLoss: 1.080 |\tPSNR: 23.001 |\tMSE: 0.005 |\ty_mse : 0.02 |\tcommit : 0.10 |\tusage : 0.16 |\tCEL : 3.45 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "21: \tLoss: 1.031 |\tPSNR: 23.274 |\tMSE: 0.005 |\ty_mse : 0.02 |\tcommit : 0.11 |\tusage : 0.18 |\tCEL : 3.68 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "22: \tLoss: 1.020 |\tPSNR: 23.380 |\tMSE: 0.005 |\ty_mse : 0.02 |\tcommit : 0.12 |\tusage : 0.19 |\tCEL : 3.81 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "23: \tLoss: 1.032 |\tPSNR: 23.313 |\tMSE: 0.005 |\ty_mse : 0.02 |\tcommit : 0.12 |\tusage : 0.19 |\tCEL : 3.66 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "24: \tLoss: 0.987 |\tPSNR: 23.552 |\tMSE: 0.004 |\ty_mse : 0.02 |\tcommit : 0.13 |\tusage : 0.20 |\tCEL : 3.72 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "25: \tLoss: 0.977 |\tPSNR: 23.565 |\tMSE: 0.004 |\ty_mse : 0.02 |\tcommit : 0.12 |\tusage : 0.20 |\tCEL : 3.65 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "26: \tLoss: 0.967 |\tPSNR: 23.638 |\tMSE: 0.004 |\ty_mse : 0.02 |\tcommit : 0.12 |\tusage : 0.20 |\tCEL : 3.69 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "27: \tLoss: 0.980 |\tPSNR: 23.559 |\tMSE: 0.004 |\ty_mse : 0.02 |\tcommit : 0.12 |\tusage : 0.20 |\tCEL : 3.65 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "28: \tLoss: 0.991 |\tPSNR: 23.519 |\tMSE: 0.004 |\ty_mse : 0.02 |\tcommit : 0.12 |\tusage : 0.20 |\tCEL : 3.72 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "29: \tLoss: 0.970 |\tPSNR: 23.676 |\tMSE: 0.004 |\ty_mse : 0.02 |\tcommit : 0.13 |\tusage : 0.21 |\tCEL : 3.83 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "30: \tLoss: 0.941 |\tPSNR: 23.839 |\tMSE: 0.004 |\ty_mse : 0.02 |\tcommit : 0.14 |\tusage : 0.20 |\tCEL : 3.84 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "31: \tLoss: 1.022 |\tPSNR: 23.478 |\tMSE: 0.004 |\ty_mse : 0.02 |\tcommit : 0.15 |\tusage : 0.20 |\tCEL : 4.04 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "32: \tLoss: 0.942 |\tPSNR: 23.889 |\tMSE: 0.004 |\ty_mse : 0.02 |\tcommit : 0.15 |\tusage : 0.21 |\tCEL : 3.87 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "33: \tLoss: 0.927 |\tPSNR: 23.972 |\tMSE: 0.004 |\ty_mse : 0.02 |\tcommit : 0.15 |\tusage : 0.21 |\tCEL : 3.90 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "34: \tLoss: 0.932 |\tPSNR: 23.966 |\tMSE: 0.004 |\ty_mse : 0.02 |\tcommit : 0.15 |\tusage : 0.22 |\tCEL : 3.94 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "35: \tLoss: 0.934 |\tPSNR: 23.909 |\tMSE: 0.004 |\ty_mse : 0.02 |\tcommit : 0.14 |\tusage : 0.21 |\tCEL : 3.87 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "36: \tLoss: 0.919 |\tPSNR: 23.960 |\tMSE: 0.004 |\ty_mse : 0.02 |\tcommit : 0.13 |\tusage : 0.20 |\tCEL : 3.81 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "37: \tLoss: 0.943 |\tPSNR: 23.966 |\tMSE: 0.004 |\ty_mse : 0.02 |\tcommit : 0.16 |\tusage : 0.21 |\tCEL : 4.09 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "38: \tLoss: 0.963 |\tPSNR: 23.807 |\tMSE: 0.004 |\ty_mse : 0.02 |\tcommit : 0.15 |\tusage : 0.21 |\tCEL : 4.08 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "39: \tLoss: 0.904 |\tPSNR: 24.145 |\tMSE: 0.004 |\ty_mse : 0.02 |\tcommit : 0.15 |\tusage : 0.21 |\tCEL : 3.99 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "40: \tLoss: 0.905 |\tPSNR: 24.118 |\tMSE: 0.004 |\ty_mse : 0.02 |\tcommit : 0.15 |\tusage : 0.20 |\tCEL : 3.96 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "41: \tLoss: 0.926 |\tPSNR: 24.072 |\tMSE: 0.004 |\ty_mse : 0.02 |\tcommit : 0.16 |\tusage : 0.21 |\tCEL : 4.01 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "42: \tLoss: 0.906 |\tPSNR: 24.142 |\tMSE: 0.004 |\ty_mse : 0.02 |\tcommit : 0.15 |\tusage : 0.20 |\tCEL : 4.00 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "43: \tLoss: 0.898 |\tPSNR: 24.214 |\tMSE: 0.004 |\ty_mse : 0.02 |\tcommit : 0.16 |\tusage : 0.20 |\tCEL : 4.03 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "44: \tLoss: 0.895 |\tPSNR: 24.201 |\tMSE: 0.004 |\ty_mse : 0.02 |\tcommit : 0.15 |\tusage : 0.20 |\tCEL : 4.04 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "45: \tLoss: 0.924 |\tPSNR: 24.132 |\tMSE: 0.004 |\ty_mse : 0.02 |\tcommit : 0.17 |\tusage : 0.21 |\tCEL : 4.20 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "46: \tLoss: 0.909 |\tPSNR: 24.137 |\tMSE: 0.004 |\ty_mse : 0.02 |\tcommit : 0.16 |\tusage : 0.20 |\tCEL : 4.09 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "47: \tLoss: 0.951 |\tPSNR: 23.864 |\tMSE: 0.004 |\ty_mse : 0.02 |\tcommit : 0.15 |\tusage : 0.19 |\tCEL : 4.06 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "48: \tLoss: 0.906 |\tPSNR: 24.196 |\tMSE: 0.004 |\ty_mse : 0.02 |\tcommit : 0.16 |\tusage : 0.20 |\tCEL : 4.11 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "49: \tLoss: 0.880 |\tPSNR: 24.310 |\tMSE: 0.004 |\ty_mse : 0.02 |\tcommit : 0.16 |\tusage : 0.19 |\tCEL : 4.10 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "50: \tLoss: 0.887 |\tPSNR: 24.336 |\tMSE: 0.004 |\ty_mse : 0.02 |\tcommit : 0.17 |\tusage : 0.20 |\tCEL : 4.24 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "51: \tLoss: 0.900 |\tPSNR: 24.253 |\tMSE: 0.004 |\ty_mse : 0.02 |\tcommit : 0.17 |\tusage : 0.20 |\tCEL : 4.28 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "52: \tLoss: 0.891 |\tPSNR: 24.278 |\tMSE: 0.004 |\ty_mse : 0.02 |\tcommit : 0.16 |\tusage : 0.19 |\tCEL : 4.20 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "53: \tLoss: 0.876 |\tPSNR: 24.366 |\tMSE: 0.004 |\ty_mse : 0.02 |\tcommit : 0.16 |\tusage : 0.20 |\tCEL : 4.16 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "54: \tLoss: 0.891 |\tPSNR: 24.311 |\tMSE: 0.004 |\ty_mse : 0.02 |\tcommit : 0.17 |\tusage : 0.20 |\tCEL : 4.28 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "55: \tLoss: 0.885 |\tPSNR: 24.348 |\tMSE: 0.004 |\ty_mse : 0.03 |\tcommit : 0.17 |\tusage : 0.20 |\tCEL : 4.21 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "56: \tLoss: 0.883 |\tPSNR: 24.386 |\tMSE: 0.004 |\ty_mse : 0.03 |\tcommit : 0.17 |\tusage : 0.20 |\tCEL : 4.31 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "57: \tLoss: 0.867 |\tPSNR: 24.409 |\tMSE: 0.004 |\ty_mse : 0.02 |\tcommit : 0.16 |\tusage : 0.19 |\tCEL : 4.12 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "58: \tLoss: 0.923 |\tPSNR: 24.069 |\tMSE: 0.004 |\ty_mse : 0.02 |\tcommit : 0.16 |\tusage : 0.19 |\tCEL : 4.19 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "59: \tLoss: 0.886 |\tPSNR: 24.372 |\tMSE: 0.004 |\ty_mse : 0.02 |\tcommit : 0.17 |\tusage : 0.20 |\tCEL : 4.32 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "60: \tLoss: 0.886 |\tPSNR: 24.387 |\tMSE: 0.004 |\ty_mse : 0.02 |\tcommit : 0.18 |\tusage : 0.20 |\tCEL : 4.35 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "61: \tLoss: 0.888 |\tPSNR: 24.379 |\tMSE: 0.004 |\ty_mse : 0.02 |\tcommit : 0.18 |\tusage : 0.20 |\tCEL : 4.38 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "62: \tLoss: 0.888 |\tPSNR: 24.294 |\tMSE: 0.004 |\ty_mse : 0.02 |\tcommit : 0.16 |\tusage : 0.19 |\tCEL : 4.21 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "63: \tLoss: 0.867 |\tPSNR: 24.474 |\tMSE: 0.004 |\ty_mse : 0.02 |\tcommit : 0.17 |\tusage : 0.19 |\tCEL : 4.38 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "64: \tLoss: 0.885 |\tPSNR: 24.275 |\tMSE: 0.004 |\ty_mse : 0.02 |\tcommit : 0.16 |\tusage : 0.19 |\tCEL : 4.12 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "65: \tLoss: 0.864 |\tPSNR: 24.532 |\tMSE: 0.004 |\ty_mse : 0.02 |\tcommit : 0.18 |\tusage : 0.20 |\tCEL : 4.40 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "66: \tLoss: 0.866 |\tPSNR: 24.499 |\tMSE: 0.004 |\ty_mse : 0.02 |\tcommit : 0.17 |\tusage : 0.20 |\tCEL : 4.42 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "67: \tLoss: 0.856 |\tPSNR: 24.545 |\tMSE: 0.004 |\ty_mse : 0.02 |\tcommit : 0.17 |\tusage : 0.20 |\tCEL : 4.37 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "68: \tLoss: 0.856 |\tPSNR: 24.526 |\tMSE: 0.004 |\ty_mse : 0.02 |\tcommit : 0.17 |\tusage : 0.20 |\tCEL : 4.35 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "69: \tLoss: 0.882 |\tPSNR: 24.352 |\tMSE: 0.004 |\ty_mse : 0.02 |\tcommit : 0.17 |\tusage : 0.20 |\tCEL : 4.29 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "70: \tLoss: 0.863 |\tPSNR: 24.503 |\tMSE: 0.004 |\ty_mse : 0.02 |\tcommit : 0.17 |\tusage : 0.21 |\tCEL : 4.42 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "71: \tLoss: 0.880 |\tPSNR: 24.369 |\tMSE: 0.004 |\ty_mse : 0.02 |\tcommit : 0.17 |\tusage : 0.20 |\tCEL : 4.30 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "72: \tLoss: 0.849 |\tPSNR: 24.610 |\tMSE: 0.003 |\ty_mse : 0.02 |\tcommit : 0.17 |\tusage : 0.21 |\tCEL : 4.43 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "73: \tLoss: 0.859 |\tPSNR: 24.472 |\tMSE: 0.004 |\ty_mse : 0.02 |\tcommit : 0.16 |\tusage : 0.21 |\tCEL : 4.39 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "74: \tLoss: 0.889 |\tPSNR: 24.358 |\tMSE: 0.004 |\ty_mse : 0.02 |\tcommit : 0.17 |\tusage : 0.21 |\tCEL : 4.42 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "75: \tLoss: 0.838 |\tPSNR: 24.665 |\tMSE: 0.003 |\ty_mse : 0.02 |\tcommit : 0.17 |\tusage : 0.21 |\tCEL : 4.38 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "76: \tLoss: 0.847 |\tPSNR: 24.577 |\tMSE: 0.003 |\ty_mse : 0.02 |\tcommit : 0.17 |\tusage : 0.21 |\tCEL : 4.38 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "77: \tLoss: 0.849 |\tPSNR: 24.627 |\tMSE: 0.003 |\ty_mse : 0.02 |\tcommit : 0.18 |\tusage : 0.22 |\tCEL : 4.41 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "78: \tLoss: 0.839 |\tPSNR: 24.719 |\tMSE: 0.003 |\ty_mse : 0.02 |\tcommit : 0.18 |\tusage : 0.22 |\tCEL : 4.54 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "79: \tLoss: 0.853 |\tPSNR: 24.624 |\tMSE: 0.003 |\ty_mse : 0.02 |\tcommit : 0.18 |\tusage : 0.22 |\tCEL : 4.49 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "80: \tLoss: 0.847 |\tPSNR: 24.587 |\tMSE: 0.003 |\ty_mse : 0.02 |\tcommit : 0.17 |\tusage : 0.22 |\tCEL : 4.40 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "81: \tLoss: 0.834 |\tPSNR: 24.680 |\tMSE: 0.003 |\ty_mse : 0.02 |\tcommit : 0.17 |\tusage : 0.22 |\tCEL : 4.47 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "82: \tLoss: 0.836 |\tPSNR: 24.725 |\tMSE: 0.003 |\ty_mse : 0.02 |\tcommit : 0.18 |\tusage : 0.23 |\tCEL : 4.55 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "83: \tLoss: 0.828 |\tPSNR: 24.752 |\tMSE: 0.003 |\ty_mse : 0.02 |\tcommit : 0.18 |\tusage : 0.22 |\tCEL : 4.51 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "84: \tLoss: 0.813 |\tPSNR: 24.813 |\tMSE: 0.003 |\ty_mse : 0.02 |\tcommit : 0.17 |\tusage : 0.22 |\tCEL : 4.40 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "85: \tLoss: 0.865 |\tPSNR: 24.398 |\tMSE: 0.004 |\ty_mse : 0.02 |\tcommit : 0.16 |\tusage : 0.22 |\tCEL : 4.25 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "86: \tLoss: 0.817 |\tPSNR: 24.840 |\tMSE: 0.003 |\ty_mse : 0.02 |\tcommit : 0.18 |\tusage : 0.24 |\tCEL : 4.50 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "87: \tLoss: 0.826 |\tPSNR: 24.775 |\tMSE: 0.003 |\ty_mse : 0.02 |\tcommit : 0.18 |\tusage : 0.24 |\tCEL : 4.54 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "88: \tLoss: 0.813 |\tPSNR: 24.837 |\tMSE: 0.003 |\ty_mse : 0.02 |\tcommit : 0.17 |\tusage : 0.24 |\tCEL : 4.50 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "89: \tLoss: 0.805 |\tPSNR: 24.842 |\tMSE: 0.003 |\ty_mse : 0.02 |\tcommit : 0.17 |\tusage : 0.23 |\tCEL : 4.39 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "90: \tLoss: 0.821 |\tPSNR: 24.778 |\tMSE: 0.003 |\ty_mse : 0.02 |\tcommit : 0.17 |\tusage : 0.24 |\tCEL : 4.55 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "91: \tLoss: 0.837 |\tPSNR: 24.643 |\tMSE: 0.003 |\ty_mse : 0.02 |\tcommit : 0.17 |\tusage : 0.24 |\tCEL : 4.41 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "92: \tLoss: 0.796 |\tPSNR: 24.828 |\tMSE: 0.003 |\ty_mse : 0.02 |\tcommit : 0.15 |\tusage : 0.24 |\tCEL : 4.28 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "93: \tLoss: 0.798 |\tPSNR: 24.904 |\tMSE: 0.003 |\ty_mse : 0.02 |\tcommit : 0.17 |\tusage : 0.26 |\tCEL : 4.43 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "94: \tLoss: 0.799 |\tPSNR: 24.906 |\tMSE: 0.003 |\ty_mse : 0.02 |\tcommit : 0.17 |\tusage : 0.26 |\tCEL : 4.44 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "95: \tLoss: 0.786 |\tPSNR: 24.967 |\tMSE: 0.003 |\ty_mse : 0.02 |\tcommit : 0.16 |\tusage : 0.26 |\tCEL : 4.39 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "96: \tLoss: 0.804 |\tPSNR: 24.906 |\tMSE: 0.003 |\ty_mse : 0.02 |\tcommit : 0.17 |\tusage : 0.27 |\tCEL : 4.52 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "97: \tLoss: 0.795 |\tPSNR: 24.910 |\tMSE: 0.003 |\ty_mse : 0.02 |\tcommit : 0.17 |\tusage : 0.27 |\tCEL : 4.37 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "98: \tLoss: 0.788 |\tPSNR: 24.920 |\tMSE: 0.003 |\ty_mse : 0.02 |\tcommit : 0.16 |\tusage : 0.27 |\tCEL : 4.31 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "99: \tLoss: 0.781 |\tPSNR: 24.955 |\tMSE: 0.003 |\ty_mse : 0.02 |\tcommit : 0.16 |\tusage : 0.27 |\tCEL : 4.30 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "100: \tLoss: 0.791 |\tPSNR: 24.906 |\tMSE: 0.003 |\ty_mse : 0.02 |\tcommit : 0.16 |\tusage : 0.27 |\tCEL : 4.35 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "101: \tLoss: 0.778 |\tPSNR: 24.983 |\tMSE: 0.003 |\ty_mse : 0.02 |\tcommit : 0.16 |\tusage : 0.28 |\tCEL : 4.38 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "102: \tLoss: 0.777 |\tPSNR: 25.045 |\tMSE: 0.003 |\ty_mse : 0.02 |\tcommit : 0.17 |\tusage : 0.28 |\tCEL : 4.38 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "103: \tLoss: 0.810 |\tPSNR: 24.791 |\tMSE: 0.003 |\ty_mse : 0.02 |\tcommit : 0.16 |\tusage : 0.29 |\tCEL : 4.31 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "104: \tLoss: 0.774 |\tPSNR: 25.055 |\tMSE: 0.003 |\ty_mse : 0.02 |\tcommit : 0.16 |\tusage : 0.30 |\tCEL : 4.36 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "105: \tLoss: 0.775 |\tPSNR: 25.056 |\tMSE: 0.003 |\ty_mse : 0.02 |\tcommit : 0.17 |\tusage : 0.30 |\tCEL : 4.41 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "106: \tLoss: 0.774 |\tPSNR: 25.081 |\tMSE: 0.003 |\ty_mse : 0.02 |\tcommit : 0.17 |\tusage : 0.31 |\tCEL : 4.34 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "107: \tLoss: 0.797 |\tPSNR: 24.956 |\tMSE: 0.003 |\ty_mse : 0.02 |\tcommit : 0.17 |\tusage : 0.32 |\tCEL : 4.34 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "108: \tLoss: 0.775 |\tPSNR: 25.093 |\tMSE: 0.003 |\ty_mse : 0.02 |\tcommit : 0.17 |\tusage : 0.32 |\tCEL : 4.39 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "109: \tLoss: 0.768 |\tPSNR: 25.157 |\tMSE: 0.003 |\ty_mse : 0.02 |\tcommit : 0.17 |\tusage : 0.33 |\tCEL : 4.43 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "110: \tLoss: 0.789 |\tPSNR: 25.012 |\tMSE: 0.003 |\ty_mse : 0.02 |\tcommit : 0.17 |\tusage : 0.34 |\tCEL : 4.39 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "111: \tLoss: 0.779 |\tPSNR: 25.131 |\tMSE: 0.003 |\ty_mse : 0.02 |\tcommit : 0.18 |\tusage : 0.34 |\tCEL : 4.36 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "112: \tLoss: 0.785 |\tPSNR: 25.062 |\tMSE: 0.003 |\ty_mse : 0.02 |\tcommit : 0.18 |\tusage : 0.34 |\tCEL : 4.32 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "113: \tLoss: 0.786 |\tPSNR: 24.843 |\tMSE: 0.003 |\ty_mse : 0.02 |\tcommit : 0.15 |\tusage : 0.31 |\tCEL : 4.13 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "114: \tLoss: 0.760 |\tPSNR: 25.214 |\tMSE: 0.003 |\ty_mse : 0.02 |\tcommit : 0.17 |\tusage : 0.35 |\tCEL : 4.39 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "115: \tLoss: 0.767 |\tPSNR: 25.220 |\tMSE: 0.003 |\ty_mse : 0.02 |\tcommit : 0.18 |\tusage : 0.36 |\tCEL : 4.43 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "116: \tLoss: 0.760 |\tPSNR: 25.281 |\tMSE: 0.003 |\ty_mse : 0.02 |\tcommit : 0.18 |\tusage : 0.36 |\tCEL : 4.40 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "117: \tLoss: 0.764 |\tPSNR: 25.212 |\tMSE: 0.003 |\ty_mse : 0.02 |\tcommit : 0.18 |\tusage : 0.36 |\tCEL : 4.38 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "118: \tLoss: 0.748 |\tPSNR: 25.278 |\tMSE: 0.003 |\ty_mse : 0.02 |\tcommit : 0.17 |\tusage : 0.35 |\tCEL : 4.34 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "119: \tLoss: 0.750 |\tPSNR: 25.259 |\tMSE: 0.003 |\ty_mse : 0.02 |\tcommit : 0.17 |\tusage : 0.35 |\tCEL : 4.35 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "120: \tLoss: 0.750 |\tPSNR: 25.326 |\tMSE: 0.003 |\ty_mse : 0.02 |\tcommit : 0.18 |\tusage : 0.36 |\tCEL : 4.40 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "121: \tLoss: 0.765 |\tPSNR: 25.159 |\tMSE: 0.003 |\ty_mse : 0.02 |\tcommit : 0.17 |\tusage : 0.36 |\tCEL : 4.31 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "122: \tLoss: 0.745 |\tPSNR: 25.295 |\tMSE: 0.003 |\ty_mse : 0.02 |\tcommit : 0.17 |\tusage : 0.36 |\tCEL : 4.38 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "123: \tLoss: 0.793 |\tPSNR: 24.949 |\tMSE: 0.003 |\ty_mse : 0.02 |\tcommit : 0.17 |\tusage : 0.36 |\tCEL : 4.43 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "124: \tLoss: 0.745 |\tPSNR: 25.295 |\tMSE: 0.003 |\ty_mse : 0.02 |\tcommit : 0.17 |\tusage : 0.36 |\tCEL : 4.33 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "125: \tLoss: 0.736 |\tPSNR: 25.303 |\tMSE: 0.003 |\ty_mse : 0.02 |\tcommit : 0.16 |\tusage : 0.35 |\tCEL : 4.27 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "126: \tLoss: 0.768 |\tPSNR: 25.170 |\tMSE: 0.003 |\ty_mse : 0.02 |\tcommit : 0.18 |\tusage : 0.36 |\tCEL : 4.42 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "127: \tLoss: 0.749 |\tPSNR: 25.318 |\tMSE: 0.003 |\ty_mse : 0.02 |\tcommit : 0.18 |\tusage : 0.37 |\tCEL : 4.35 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "128: \tLoss: 0.732 |\tPSNR: 25.329 |\tMSE: 0.003 |\ty_mse : 0.02 |\tcommit : 0.16 |\tusage : 0.35 |\tCEL : 4.24 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "129: \tLoss: 0.747 |\tPSNR: 25.267 |\tMSE: 0.003 |\ty_mse : 0.02 |\tcommit : 0.17 |\tusage : 0.36 |\tCEL : 4.38 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "130: \tLoss: 0.748 |\tPSNR: 25.280 |\tMSE: 0.003 |\ty_mse : 0.02 |\tcommit : 0.17 |\tusage : 0.37 |\tCEL : 4.37 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "131: \tLoss: 0.742 |\tPSNR: 25.314 |\tMSE: 0.003 |\ty_mse : 0.02 |\tcommit : 0.17 |\tusage : 0.38 |\tCEL : 4.46 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "132: \tLoss: 0.742 |\tPSNR: 25.342 |\tMSE: 0.003 |\ty_mse : 0.02 |\tcommit : 0.17 |\tusage : 0.38 |\tCEL : 4.41 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "133: \tLoss: 0.735 |\tPSNR: 25.368 |\tMSE: 0.003 |\ty_mse : 0.02 |\tcommit : 0.17 |\tusage : 0.37 |\tCEL : 4.38 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "134: \tLoss: 0.736 |\tPSNR: 25.356 |\tMSE: 0.003 |\ty_mse : 0.02 |\tcommit : 0.17 |\tusage : 0.38 |\tCEL : 4.42 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "135: \tLoss: 0.738 |\tPSNR: 25.325 |\tMSE: 0.003 |\ty_mse : 0.02 |\tcommit : 0.17 |\tusage : 0.38 |\tCEL : 4.46 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "136: \tLoss: 0.742 |\tPSNR: 25.333 |\tMSE: 0.003 |\ty_mse : 0.02 |\tcommit : 0.17 |\tusage : 0.38 |\tCEL : 4.41 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "137: \tLoss: 0.745 |\tPSNR: 25.310 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.17 |\tusage : 0.39 |\tCEL : 4.42 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "138: \tLoss: 0.733 |\tPSNR: 25.405 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.17 |\tusage : 0.38 |\tCEL : 4.48 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "139: \tLoss: 0.712 |\tPSNR: 25.421 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.15 |\tusage : 0.36 |\tCEL : 4.29 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "140: \tLoss: 0.721 |\tPSNR: 25.402 |\tMSE: 0.003 |\ty_mse : 0.02 |\tcommit : 0.16 |\tusage : 0.37 |\tCEL : 4.38 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "141: \tLoss: 0.722 |\tPSNR: 25.477 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.17 |\tusage : 0.38 |\tCEL : 4.44 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "142: \tLoss: 0.724 |\tPSNR: 25.477 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.17 |\tusage : 0.38 |\tCEL : 4.48 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "143: \tLoss: 0.791 |\tPSNR: 24.912 |\tMSE: 0.003 |\ty_mse : 0.02 |\tcommit : 0.16 |\tusage : 0.37 |\tCEL : 4.30 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "144: \tLoss: 0.718 |\tPSNR: 25.452 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.16 |\tusage : 0.37 |\tCEL : 4.39 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "145: \tLoss: 0.730 |\tPSNR: 25.419 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.17 |\tusage : 0.38 |\tCEL : 4.47 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "146: \tLoss: 0.720 |\tPSNR: 25.479 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.17 |\tusage : 0.38 |\tCEL : 4.51 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "147: \tLoss: 0.731 |\tPSNR: 25.430 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.17 |\tusage : 0.38 |\tCEL : 4.45 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "148: \tLoss: 0.718 |\tPSNR: 25.442 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.16 |\tusage : 0.37 |\tCEL : 4.35 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "149: \tLoss: 0.714 |\tPSNR: 25.516 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.17 |\tusage : 0.37 |\tCEL : 4.45 |\tBpp: 0.01 |\n",
      "Learning rate: 0.0001\n",
      "150: \tLoss: 0.731 |\tPSNR: 25.366 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.16 |\tusage : 0.37 |\tCEL : 4.44 |\tBpp: 0.01 |\n",
      "Learning rate: 1e-05\n",
      "151: \tLoss: 0.704 |\tPSNR: 25.597 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.17 |\tusage : 0.38 |\tCEL : 4.46 |\tBpp: 0.01 |\n",
      "Learning rate: 1e-05\n",
      "152: \tLoss: 0.705 |\tPSNR: 25.600 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.17 |\tusage : 0.38 |\tCEL : 4.51 |\tBpp: 0.01 |\n",
      "Learning rate: 1e-05\n",
      "153: \tLoss: 0.703 |\tPSNR: 25.608 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.17 |\tusage : 0.38 |\tCEL : 4.49 |\tBpp: 0.01 |\n",
      "Learning rate: 1e-05\n",
      "154: \tLoss: 0.704 |\tPSNR: 25.604 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.17 |\tusage : 0.38 |\tCEL : 4.49 |\tBpp: 0.01 |\n",
      "Learning rate: 1e-05\n",
      "155: \tLoss: 0.703 |\tPSNR: 25.611 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.17 |\tusage : 0.38 |\tCEL : 4.50 |\tBpp: 0.01 |\n",
      "Learning rate: 1e-05\n",
      "156: \tLoss: 0.702 |\tPSNR: 25.612 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.17 |\tusage : 0.38 |\tCEL : 4.49 |\tBpp: 0.01 |\n",
      "Learning rate: 1e-05\n",
      "157: \tLoss: 0.700 |\tPSNR: 25.606 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.16 |\tusage : 0.37 |\tCEL : 4.48 |\tBpp: 0.01 |\n",
      "Learning rate: 1e-05\n",
      "158: \tLoss: 0.700 |\tPSNR: 25.615 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.17 |\tusage : 0.38 |\tCEL : 4.49 |\tBpp: 0.01 |\n",
      "Learning rate: 1e-05\n",
      "159: \tLoss: 0.705 |\tPSNR: 25.618 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.17 |\tusage : 0.38 |\tCEL : 4.55 |\tBpp: 0.01 |\n",
      "Learning rate: 1e-05\n",
      "160: \tLoss: 0.703 |\tPSNR: 25.624 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.17 |\tusage : 0.38 |\tCEL : 4.53 |\tBpp: 0.01 |\n",
      "Learning rate: 1e-05\n",
      "161: \tLoss: 0.697 |\tPSNR: 25.621 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.16 |\tusage : 0.37 |\tCEL : 4.49 |\tBpp: 0.01 |\n",
      "Learning rate: 1e-05\n",
      "162: \tLoss: 0.703 |\tPSNR: 25.627 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.17 |\tusage : 0.38 |\tCEL : 4.57 |\tBpp: 0.01 |\n",
      "Learning rate: 1e-05\n",
      "163: \tLoss: 0.699 |\tPSNR: 25.629 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.16 |\tusage : 0.38 |\tCEL : 4.50 |\tBpp: 0.01 |\n",
      "Learning rate: 1e-05\n",
      "164: \tLoss: 0.699 |\tPSNR: 25.628 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.17 |\tusage : 0.38 |\tCEL : 4.54 |\tBpp: 0.01 |\n",
      "Learning rate: 1e-05\n",
      "165: \tLoss: 0.698 |\tPSNR: 25.612 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.16 |\tusage : 0.37 |\tCEL : 4.49 |\tBpp: 0.01 |\n",
      "Learning rate: 1e-05\n",
      "166: \tLoss: 0.694 |\tPSNR: 25.633 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.16 |\tusage : 0.37 |\tCEL : 4.47 |\tBpp: 0.01 |\n",
      "Learning rate: 1e-05\n",
      "167: \tLoss: 0.700 |\tPSNR: 25.634 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.17 |\tusage : 0.38 |\tCEL : 4.52 |\tBpp: 0.01 |\n",
      "Learning rate: 1e-05\n",
      "168: \tLoss: 0.697 |\tPSNR: 25.640 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.16 |\tusage : 0.38 |\tCEL : 4.53 |\tBpp: 0.01 |\n",
      "Learning rate: 1e-05\n",
      "169: \tLoss: 0.699 |\tPSNR: 25.655 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.17 |\tusage : 0.38 |\tCEL : 4.57 |\tBpp: 0.01 |\n",
      "Learning rate: 1e-05\n",
      "170: \tLoss: 0.702 |\tPSNR: 25.637 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.17 |\tusage : 0.38 |\tCEL : 4.60 |\tBpp: 0.01 |\n",
      "Learning rate: 1e-05\n",
      "171: \tLoss: 0.696 |\tPSNR: 25.634 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.16 |\tusage : 0.37 |\tCEL : 4.53 |\tBpp: 0.01 |\n",
      "Learning rate: 1e-05\n",
      "172: \tLoss: 0.701 |\tPSNR: 25.649 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.17 |\tusage : 0.38 |\tCEL : 4.60 |\tBpp: 0.01 |\n",
      "Learning rate: 1e-05\n",
      "173: \tLoss: 0.697 |\tPSNR: 25.647 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.17 |\tusage : 0.38 |\tCEL : 4.54 |\tBpp: 0.01 |\n",
      "Learning rate: 1e-05\n",
      "174: \tLoss: 0.701 |\tPSNR: 25.641 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.17 |\tusage : 0.38 |\tCEL : 4.60 |\tBpp: 0.01 |\n",
      "Learning rate: 1e-05\n",
      "175: \tLoss: 0.692 |\tPSNR: 25.656 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.16 |\tusage : 0.37 |\tCEL : 4.50 |\tBpp: 0.01 |\n",
      "Learning rate: 1e-05\n",
      "176: \tLoss: 0.704 |\tPSNR: 25.658 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.17 |\tusage : 0.39 |\tCEL : 4.66 |\tBpp: 0.01 |\n",
      "Learning rate: 1e-05\n",
      "177: \tLoss: 0.707 |\tPSNR: 25.628 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.17 |\tusage : 0.39 |\tCEL : 4.68 |\tBpp: 0.01 |\n",
      "Learning rate: 1e-05\n",
      "178: \tLoss: 0.694 |\tPSNR: 25.645 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.16 |\tusage : 0.37 |\tCEL : 4.50 |\tBpp: 0.01 |\n",
      "Learning rate: 1e-05\n",
      "179: \tLoss: 0.700 |\tPSNR: 25.650 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.17 |\tusage : 0.39 |\tCEL : 4.68 |\tBpp: 0.01 |\n",
      "Learning rate: 1e-05\n",
      "180: \tLoss: 0.695 |\tPSNR: 25.666 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.17 |\tusage : 0.38 |\tCEL : 4.57 |\tBpp: 0.01 |\n",
      "Learning rate: 1e-05\n",
      "181: \tLoss: 0.692 |\tPSNR: 25.665 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.16 |\tusage : 0.37 |\tCEL : 4.53 |\tBpp: 0.01 |\n",
      "Learning rate: 1e-05\n",
      "182: \tLoss: 0.692 |\tPSNR: 25.664 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.16 |\tusage : 0.37 |\tCEL : 4.53 |\tBpp: 0.01 |\n",
      "Learning rate: 1e-05\n",
      "183: \tLoss: 0.693 |\tPSNR: 25.671 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.16 |\tusage : 0.38 |\tCEL : 4.58 |\tBpp: 0.01 |\n",
      "Learning rate: 1e-05\n",
      "184: \tLoss: 0.691 |\tPSNR: 25.668 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.16 |\tusage : 0.38 |\tCEL : 4.54 |\tBpp: 0.01 |\n",
      "Learning rate: 1e-05\n",
      "185: \tLoss: 0.694 |\tPSNR: 25.664 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.16 |\tusage : 0.38 |\tCEL : 4.53 |\tBpp: 0.01 |\n",
      "Learning rate: 1e-05\n",
      "186: \tLoss: 0.696 |\tPSNR: 25.675 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.17 |\tusage : 0.39 |\tCEL : 4.60 |\tBpp: 0.01 |\n",
      "Learning rate: 1e-05\n",
      "187: \tLoss: 0.697 |\tPSNR: 25.664 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.17 |\tusage : 0.39 |\tCEL : 4.60 |\tBpp: 0.01 |\n",
      "Learning rate: 1e-05\n",
      "188: \tLoss: 0.693 |\tPSNR: 25.675 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.17 |\tusage : 0.38 |\tCEL : 4.60 |\tBpp: 0.01 |\n",
      "Learning rate: 1e-05\n",
      "189: \tLoss: 0.701 |\tPSNR: 25.675 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.17 |\tusage : 0.40 |\tCEL : 4.69 |\tBpp: 0.01 |\n",
      "Learning rate: 1e-05\n",
      "190: \tLoss: 0.692 |\tPSNR: 25.674 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.16 |\tusage : 0.38 |\tCEL : 4.59 |\tBpp: 0.01 |\n",
      "Learning rate: 1e-05\n",
      "191: \tLoss: 0.697 |\tPSNR: 25.678 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.17 |\tusage : 0.39 |\tCEL : 4.64 |\tBpp: 0.01 |\n",
      "Learning rate: 1e-05\n",
      "192: \tLoss: 0.697 |\tPSNR: 25.676 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.17 |\tusage : 0.39 |\tCEL : 4.70 |\tBpp: 0.01 |\n",
      "Learning rate: 1e-05\n",
      "193: \tLoss: 0.694 |\tPSNR: 25.672 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.17 |\tusage : 0.38 |\tCEL : 4.63 |\tBpp: 0.01 |\n",
      "Learning rate: 1e-05\n",
      "194: \tLoss: 0.699 |\tPSNR: 25.671 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.17 |\tusage : 0.39 |\tCEL : 4.69 |\tBpp: 0.01 |\n",
      "Learning rate: 1e-05\n",
      "195: \tLoss: 0.697 |\tPSNR: 25.677 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.17 |\tusage : 0.39 |\tCEL : 4.68 |\tBpp: 0.01 |\n",
      "Learning rate: 1.0000000000000002e-06\n",
      "196: \tLoss: 0.687 |\tPSNR: 25.688 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.16 |\tusage : 0.37 |\tCEL : 4.54 |\tBpp: 0.01 |\n",
      "Learning rate: 1.0000000000000002e-06\n",
      "197: \tLoss: 0.692 |\tPSNR: 25.694 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.17 |\tusage : 0.38 |\tCEL : 4.62 |\tBpp: 0.01 |\n",
      "Learning rate: 1.0000000000000002e-06\n",
      "198: \tLoss: 0.688 |\tPSNR: 25.691 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.16 |\tusage : 0.37 |\tCEL : 4.54 |\tBpp: 0.01 |\n",
      "Learning rate: 1.0000000000000002e-06\n",
      "199: \tLoss: 0.691 |\tPSNR: 25.691 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.17 |\tusage : 0.38 |\tCEL : 4.61 |\tBpp: 0.01 |\n",
      "Learning rate: 1.0000000000000002e-06\n",
      "200: \tLoss: 0.688 |\tPSNR: 25.690 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.16 |\tusage : 0.38 |\tCEL : 4.55 |\tBpp: 0.01 |\n",
      "Learning rate: 1.0000000000000002e-06\n",
      "201: \tLoss: 0.693 |\tPSNR: 25.687 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.17 |\tusage : 0.39 |\tCEL : 4.65 |\tBpp: 0.01 |\n",
      "Learning rate: 1.0000000000000002e-06\n",
      "202: \tLoss: 0.687 |\tPSNR: 25.693 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.16 |\tusage : 0.37 |\tCEL : 4.55 |\tBpp: 0.01 |\n",
      "Learning rate: 1.0000000000000002e-06\n",
      "203: \tLoss: 0.692 |\tPSNR: 25.690 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.17 |\tusage : 0.38 |\tCEL : 4.62 |\tBpp: 0.01 |\n",
      "Learning rate: 1.0000000000000002e-06\n",
      "204: \tLoss: 0.698 |\tPSNR: 25.695 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.17 |\tusage : 0.40 |\tCEL : 4.71 |\tBpp: 0.01 |\n",
      "Learning rate: 1.0000000000000002e-06\n",
      "205: \tLoss: 0.690 |\tPSNR: 25.698 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.16 |\tusage : 0.38 |\tCEL : 4.61 |\tBpp: 0.01 |\n",
      "Learning rate: 1.0000000000000002e-06\n",
      "206: \tLoss: 0.695 |\tPSNR: 25.690 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.17 |\tusage : 0.39 |\tCEL : 4.68 |\tBpp: 0.01 |\n",
      "Learning rate: 1.0000000000000002e-06\n",
      "207: \tLoss: 0.688 |\tPSNR: 25.696 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.16 |\tusage : 0.38 |\tCEL : 4.59 |\tBpp: 0.01 |\n",
      "Learning rate: 1.0000000000000002e-06\n",
      "208: \tLoss: 0.692 |\tPSNR: 25.706 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.17 |\tusage : 0.39 |\tCEL : 4.66 |\tBpp: 0.01 |\n",
      "Learning rate: 1.0000000000000002e-06\n",
      "209: \tLoss: 0.691 |\tPSNR: 25.702 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.17 |\tusage : 0.39 |\tCEL : 4.65 |\tBpp: 0.01 |\n",
      "Learning rate: 1.0000000000000002e-06\n",
      "210: \tLoss: 0.693 |\tPSNR: 25.696 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.17 |\tusage : 0.39 |\tCEL : 4.66 |\tBpp: 0.01 |\n",
      "Learning rate: 1.0000000000000002e-06\n",
      "211: \tLoss: 0.696 |\tPSNR: 25.696 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.17 |\tusage : 0.40 |\tCEL : 4.69 |\tBpp: 0.01 |\n",
      "Learning rate: 1.0000000000000002e-06\n",
      "212: \tLoss: 0.689 |\tPSNR: 25.708 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.16 |\tusage : 0.39 |\tCEL : 4.62 |\tBpp: 0.01 |\n",
      "Learning rate: 1.0000000000000002e-06\n",
      "213: \tLoss: 0.689 |\tPSNR: 25.708 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.17 |\tusage : 0.39 |\tCEL : 4.63 |\tBpp: 0.01 |\n",
      "Learning rate: 1.0000000000000002e-07\n",
      "214: \tLoss: 0.691 |\tPSNR: 25.708 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.17 |\tusage : 0.39 |\tCEL : 4.65 |\tBpp: 0.01 |\n",
      "Learning rate: 1.0000000000000002e-07\n",
      "215: \tLoss: 0.692 |\tPSNR: 25.705 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.17 |\tusage : 0.39 |\tCEL : 4.67 |\tBpp: 0.01 |\n",
      "Learning rate: 1.0000000000000002e-07\n",
      "216: \tLoss: 0.694 |\tPSNR: 25.706 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.17 |\tusage : 0.40 |\tCEL : 4.71 |\tBpp: 0.01 |\n",
      "Learning rate: 1.0000000000000002e-07\n",
      "217: \tLoss: 0.688 |\tPSNR: 25.712 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.16 |\tusage : 0.39 |\tCEL : 4.62 |\tBpp: 0.01 |\n",
      "Learning rate: 1.0000000000000002e-07\n",
      "218: \tLoss: 0.687 |\tPSNR: 25.710 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.16 |\tusage : 0.39 |\tCEL : 4.60 |\tBpp: 0.01 |\n",
      "Learning rate: 1.0000000000000002e-07\n",
      "219: \tLoss: 0.688 |\tPSNR: 25.715 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.16 |\tusage : 0.39 |\tCEL : 4.62 |\tBpp: 0.01 |\n",
      "Learning rate: 1.0000000000000002e-07\n",
      "220: \tLoss: 0.689 |\tPSNR: 25.711 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.17 |\tusage : 0.39 |\tCEL : 4.63 |\tBpp: 0.01 |\n",
      "Learning rate: 1.0000000000000002e-07\n",
      "221: \tLoss: 0.686 |\tPSNR: 25.705 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.16 |\tusage : 0.38 |\tCEL : 4.58 |\tBpp: 0.01 |\n",
      "Learning rate: 1.0000000000000002e-07\n",
      "222: \tLoss: 0.692 |\tPSNR: 25.709 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.17 |\tusage : 0.39 |\tCEL : 4.69 |\tBpp: 0.01 |\n",
      "Learning rate: 1.0000000000000002e-07\n",
      "223: \tLoss: 0.687 |\tPSNR: 25.711 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.16 |\tusage : 0.39 |\tCEL : 4.63 |\tBpp: 0.01 |\n",
      "Learning rate: 1.0000000000000002e-07\n",
      "224: \tLoss: 0.684 |\tPSNR: 25.710 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.16 |\tusage : 0.38 |\tCEL : 4.55 |\tBpp: 0.01 |\n",
      "Learning rate: 1.0000000000000002e-07\n",
      "225: \tLoss: 0.684 |\tPSNR: 25.707 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.16 |\tusage : 0.38 |\tCEL : 4.54 |\tBpp: 0.01 |\n",
      "Learning rate: 1.0000000000000002e-07\n",
      "226: \tLoss: 0.687 |\tPSNR: 25.718 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.16 |\tusage : 0.39 |\tCEL : 4.63 |\tBpp: 0.01 |\n",
      "Learning rate: 1.0000000000000002e-07\n",
      "227: \tLoss: 0.685 |\tPSNR: 25.707 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.16 |\tusage : 0.38 |\tCEL : 4.59 |\tBpp: 0.01 |\n",
      "Learning rate: 1.0000000000000002e-07\n",
      "228: \tLoss: 0.685 |\tPSNR: 25.715 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.16 |\tusage : 0.38 |\tCEL : 4.59 |\tBpp: 0.01 |\n",
      "Learning rate: 1.0000000000000002e-07\n",
      "229: \tLoss: 0.689 |\tPSNR: 25.713 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.17 |\tusage : 0.39 |\tCEL : 4.65 |\tBpp: 0.01 |\n",
      "Learning rate: 1.0000000000000002e-07\n",
      "230: \tLoss: 0.685 |\tPSNR: 25.718 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.16 |\tusage : 0.39 |\tCEL : 4.60 |\tBpp: 0.01 |\n",
      "Learning rate: 1.0000000000000002e-07\n",
      "231: \tLoss: 0.686 |\tPSNR: 25.723 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.16 |\tusage : 0.39 |\tCEL : 4.62 |\tBpp: 0.01 |\n",
      "Learning rate: 1.0000000000000002e-07\n",
      "232: \tLoss: 0.686 |\tPSNR: 25.721 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.16 |\tusage : 0.39 |\tCEL : 4.64 |\tBpp: 0.01 |\n",
      "Learning rate: 1.0000000000000002e-07\n",
      "233: \tLoss: 0.682 |\tPSNR: 25.718 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.16 |\tusage : 0.38 |\tCEL : 4.57 |\tBpp: 0.01 |\n",
      "Learning rate: 1.0000000000000002e-07\n",
      "234: \tLoss: 0.686 |\tPSNR: 25.718 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.16 |\tusage : 0.39 |\tCEL : 4.63 |\tBpp: 0.01 |\n",
      "Learning rate: 1.0000000000000002e-07\n",
      "235: \tLoss: 0.685 |\tPSNR: 25.723 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.16 |\tusage : 0.39 |\tCEL : 4.63 |\tBpp: 0.01 |\n",
      "Learning rate: 1.0000000000000002e-07\n",
      "236: \tLoss: 0.683 |\tPSNR: 25.724 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.16 |\tusage : 0.39 |\tCEL : 4.62 |\tBpp: 0.01 |\n",
      "Learning rate: 1.0000000000000002e-07\n",
      "237: \tLoss: 0.682 |\tPSNR: 25.722 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.16 |\tusage : 0.38 |\tCEL : 4.57 |\tBpp: 0.01 |\n",
      "Learning rate: 1.0000000000000002e-07\n",
      "238: \tLoss: 0.689 |\tPSNR: 25.722 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.17 |\tusage : 0.40 |\tCEL : 4.69 |\tBpp: 0.01 |\n",
      "Learning rate: 1.0000000000000002e-07\n",
      "239: \tLoss: 0.690 |\tPSNR: 25.711 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.17 |\tusage : 0.40 |\tCEL : 4.67 |\tBpp: 0.01 |\n",
      "Learning rate: 1.0000000000000002e-07\n",
      "240: \tLoss: 0.685 |\tPSNR: 25.727 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.16 |\tusage : 0.39 |\tCEL : 4.63 |\tBpp: 0.01 |\n",
      "Learning rate: 1.0000000000000002e-07\n",
      "241: \tLoss: 0.681 |\tPSNR: 25.718 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.16 |\tusage : 0.38 |\tCEL : 4.54 |\tBpp: 0.01 |\n",
      "Learning rate: 1.0000000000000002e-07\n",
      "242: \tLoss: 0.685 |\tPSNR: 25.728 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.16 |\tusage : 0.39 |\tCEL : 4.63 |\tBpp: 0.01 |\n",
      "Learning rate: 1.0000000000000002e-07\n",
      "243: \tLoss: 0.685 |\tPSNR: 25.725 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.16 |\tusage : 0.39 |\tCEL : 4.62 |\tBpp: 0.01 |\n",
      "Learning rate: 1.0000000000000002e-07\n",
      "244: \tLoss: 0.686 |\tPSNR: 25.723 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.16 |\tusage : 0.40 |\tCEL : 4.66 |\tBpp: 0.01 |\n",
      "Learning rate: 1.0000000000000002e-07\n",
      "245: \tLoss: 0.682 |\tPSNR: 25.720 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.16 |\tusage : 0.39 |\tCEL : 4.57 |\tBpp: 0.01 |\n",
      "Learning rate: 1.0000000000000002e-07\n",
      "246: \tLoss: 0.685 |\tPSNR: 25.726 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.16 |\tusage : 0.40 |\tCEL : 4.66 |\tBpp: 0.01 |\n",
      "Learning rate: 1.0000000000000002e-07\n",
      "247: \tLoss: 0.682 |\tPSNR: 25.720 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.16 |\tusage : 0.39 |\tCEL : 4.57 |\tBpp: 0.01 |\n",
      "Learning rate: 1.0000000000000002e-07\n",
      "248: \tLoss: 0.687 |\tPSNR: 25.729 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.17 |\tusage : 0.40 |\tCEL : 4.67 |\tBpp: 0.01 |\n",
      "Learning rate: 1.0000000000000002e-07\n",
      "249: \tLoss: 0.685 |\tPSNR: 25.733 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.16 |\tusage : 0.39 |\tCEL : 4.64 |\tBpp: 0.01 |\n",
      "Learning rate: 1.0000000000000002e-07\n",
      "250: \tLoss: 0.687 |\tPSNR: 25.729 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.17 |\tusage : 0.40 |\tCEL : 4.69 |\tBpp: 0.01 |\n",
      "Learning rate: 1.0000000000000002e-07\n",
      "251: \tLoss: 0.684 |\tPSNR: 25.727 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.16 |\tusage : 0.39 |\tCEL : 4.64 |\tBpp: 0.01 |\n",
      "Learning rate: 1.0000000000000002e-07\n",
      "252: \tLoss: 0.684 |\tPSNR: 25.728 |\tMSE: 0.003 |\ty_mse : 0.03 |\tcommit : 0.16 |\tusage : 0.39 |\tCEL : 4.62 |\tBpp: 0.01 |\n"
     ]
    }
   ],
   "source": [
    "# (self, N=128, quantizers=1, CB_size=512, dim=64, **kwargs):\n",
    "net = Hyper_VQ(128, quantizers=quantizers, CB_size=CB_size_setting, dim=dim).to(device)\n",
    "optimizer, hyper_optimizer, aux_optimizer = configure_optimizers(net)\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"min\")\n",
    "criterion = RateDistortionLoss(lmbda=lambda_setting)\n",
    "\n",
    "path = 'vq.tar'\n",
    "last_epoch = 0\n",
    "if False:\n",
    "    print(\"Loading\", path)\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    last_epoch = checkpoint[\"epoch\"] + 1\n",
    "    net.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "    aux_optimizer.load_state_dict(checkpoint[\"aux_optimizer\"])\n",
    "    lr_scheduler.load_state_dict(checkpoint[\"lr_scheduler\"])\n",
    "\n",
    "with open(f'{log_path}.csv','a') as f:\n",
    "    log_s = f\"epochs, PSNR, MSE, y_mse, commit, usage, CE, bpp\\n\"\n",
    "    f.write(log_s)\n",
    "\n",
    "best_loss = float(\"inf\")\n",
    "for epoch in range(last_epoch, epochs):\n",
    "    lring = optimizer.param_groups[0]['lr']\n",
    "    if lring < 1e-7:\n",
    "        break\n",
    "    print(f\"Learning rate: {lring}\")\n",
    "    train_one_epoch(\n",
    "        net,\n",
    "        criterion,\n",
    "        train_dataloader,\n",
    "        optimizer,\n",
    "        hyper_optimizer,\n",
    "        aux_optimizer,\n",
    "        epoch,\n",
    "        1,\n",
    "    )\n",
    "    loss = test_epoch(epoch, test_dataloader, net, criterion)\n",
    "    lr_scheduler.step(loss)\n",
    "\n",
    "    is_best = loss < best_loss\n",
    "    best_loss = min(loss, best_loss)\n",
    "\n",
    "    save_checkpoint(\n",
    "        {\n",
    "            \"epoch\": epoch,\n",
    "            \"state_dict\": net.state_dict(),\n",
    "            \"loss\": loss,\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"aux_optimizer\": aux_optimizer.state_dict(),\n",
    "            \"lr_scheduler\": lr_scheduler.state_dict(),\n",
    "        },\n",
    "        is_best,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.339914133584916, 0.9427255988121033, 0.010561015456914902\n",
      "\n"
     ]
    }
   ],
   "source": [
    "psnr, msssim, zbpp = eval_image(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.73177193237648\n"
     ]
    }
   ],
   "source": [
    "model = net\n",
    "with torch.no_grad():\n",
    "    for d in test_dataloader:\n",
    "        d = d.to(device)\n",
    "\n",
    "        y = model.g_a(d)\n",
    "        if hyper_enable:\n",
    "            y_std, ce, z_likelihoods, scales_hat, means_hat = model.represent_befor_quantize(y)\n",
    "        else:\n",
    "            y_std = y\n",
    "            ce = model.calc_cross_entropy(y)\n",
    "            z_likelihoods = torch.tensor(0)\n",
    "\n",
    "        y_hat, id, commit, usage = model.vq(y_std)  # (b, Q, w, h), (b, Q, w, h), (b), (b)\n",
    "        y_hat_ = model.destandardized(y_hat, scales_hat, means_hat) if hyper_enable else y_hat\n",
    "        x_hat = model.g_s(y_hat_)\n",
    "\n",
    "x_hat_ =  x_hat.clamp(0, 1)\n",
    "print(compute_psnr(x_hat_, d))        \n",
    "b, c, h, w = d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_and_sigma(symbol):\n",
    "    symbol_flat = symbol.view(-1)    \n",
    "    mean = symbol_flat.mean()\n",
    "    mean_flat = torch.zeros_like(symbol_flat) + mean    \n",
    "    sigma = ((symbol_flat - mean_flat) ** 2).mean()\n",
    "    return mean, sigma\n",
    "m, s = get_mean_and_sigma(y)\n",
    "m_, s_ = get_mean_and_sigma(y_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10857974737882614, 0.2465667575597763; 0.17236687242984772\n",
      "0.10791861265897751, 0.1928798407316208; 0.1199619397521019\n"
     ]
    }
   ],
   "source": [
    "print(f'{scales_hat.min()}, {scales_hat.max()}; {scales_hat.mean()}')\n",
    "print(f'{means_hat.min()}, {means_hat.max()}; {means_hat.mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_CE(symbol):\n",
    "    CE = net.calc_cross_entropy(symbol)\n",
    "    CE_avg = torch.log(CE).sum() / (-math.log(2) * (b * h * w))\n",
    "    return CE_avg.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",mse wt, 0.003\n",
      ",yCE, 1.4798552989959717\n",
      ",y_CE, 4.618378639221191\n",
      ",ymu, 0.05455763638019562\n",
      ",ysigma, 0.14016319811344147\n",
      ",y_mu, -0.38374555110931396\n",
      ",y_sigma, 5.324619293212891\n",
      ",PSNR, 28.339914133584916\n",
      ",ms-ssim, 0.9427255988121033\n",
      ",usage, 0.3918726444244385\n",
      ",zbpp, 0.010561015456914902\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_s = f\",mse wt, {lambda_setting}\\n\"\n",
    "log_s = log_s + f\",yCE, {get_CE(y)}\\n\"\n",
    "log_s = log_s + f\",y_CE, {get_CE(y_std)}\\n\"\n",
    "log_s = log_s + f\",ymu, {m}\\n\"\n",
    "log_s = log_s + f\",ysigma, {s}\\n\"\n",
    "log_s = log_s + f\",y_mu, {m_}\\n\"\n",
    "log_s = log_s + f\",y_sigma, {s_}\\n\"\n",
    "log_s = log_s + f\",PSNR, {psnr}\\n\"\n",
    "log_s = log_s + f\",ms-ssim, {msssim}\\n\"\n",
    "log_s = log_s + f\",usage, {usage.item()}\\n\"\n",
    "log_s = log_s + f\",zbpp, {zbpp}\\n\"\n",
    "print(log_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{log_path}.csv','a') as f:\n",
    "    f.write(log_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([18, 64, 64, 96])\n",
      "1.4199329614639282\n"
     ]
    }
   ],
   "source": [
    "def get_CE(symbol):\n",
    "    CE = net.calc_cross_entropy(symbol)\n",
    "    CE_avg = torch.log(CE).sum() / (-(b * h * w))\n",
    "    return CE_avg.item()\n",
    "\n",
    "xxx = torch.ones_like(y)\n",
    "print(xxx.shape)\n",
    "print(get_CE(xxx))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "4b3695a85a4caa0373e4268d79926f2001c5518f41e76a28112350d8a4ba6e82"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
