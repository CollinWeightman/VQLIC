{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vector_quantize_pytorch.vector_quantize_pytorch import VectorQuantize\n",
    "from vector_quantize_pytorch.residual_vq import ResidualVQ, MultiLayerVQ, HierarchicalVQ\n",
    "\n",
    "from compressai.models.utils import conv, deconv, update_registered_buffers\n",
    "from compressai.layers import GDN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import warnings\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from compressai.models import ScaleHyperprior\n",
    "from compressai.entropy_models import EntropyBottleneck, GaussianConditional\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "from torch import Tensor\n",
    "\n",
    "from typing import Any, Callable, List, Optional, Tuple, Union\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from compressai.datasets import ImageFolder\n",
    "import torch.optim as optim\n",
    "import shutil\n",
    "from pytorch_msssim import ms_ssim\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "from compressai.layers import (\n",
    "    AttentionBlock,\n",
    "    ResidualBlock,\n",
    "    ResidualBlockUpsample,\n",
    "    ResidualBlockWithStride,\n",
    "    conv3x3,\n",
    "    subpel_conv3x3,\n",
    ")\n",
    "from compressai.ops import LowerBound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_enable = False\n",
    "ce_loss_enable = False\n",
    "element_wise = True\n",
    "Hierarchical_architecture = True\n",
    "\n",
    "log_path = \"log\"\n",
    "version = \"test\"\n",
    "lambda_setting = 0.003\n",
    "quantizers = 4\n",
    "CB_size_setting = 256\n",
    "workers_setting = 4\n",
    "batch_size_setting = 8\n",
    "epochs = 1000\n",
    "loop = 0\n",
    "times = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hyper_VQ(ScaleHyperprior):\n",
    "    def __init__(self, N=128, quantizers=1, CB_size=512, dim=64, **kwargs):\n",
    "        super().__init__(N, N, **kwargs)\n",
    "        self.gaussian_conditional = GaussianConditional(None)\n",
    "\n",
    "        self.lower_bound_l = LowerBound(1e-9)\n",
    "        self.lower_bound_s = LowerBound(0.11)\n",
    "        self.quantizers = quantizers\n",
    "        \n",
    "        if Hierarchical_architecture:\n",
    "            dim_list = dim\n",
    "            dim = sum(dim)\n",
    "        \n",
    "        self.dim = dim\n",
    "        self.g_a = nn.Sequential(\n",
    "            ResidualBlockWithStride(3, N, stride=2),\n",
    "            AttentionBlock(N),\n",
    "            ResidualBlock(N, N),\n",
    "            ResidualBlockWithStride(N, N, stride=2),\n",
    "            AttentionBlock(N),\n",
    "            ResidualBlock(N, N),\n",
    "            conv3x3(N, dim, stride=2),\n",
    "            AttentionBlock(dim),\n",
    "            ResidualBlock(dim, dim),\n",
    "        )\n",
    "        self.h_a = nn.Sequential(\n",
    "            conv3x3(dim, N),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            conv3x3(N, N),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            conv3x3(N, N, stride=2),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            conv3x3(N, N),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            conv3x3(N, N, stride=2),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            conv3x3(N, N),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            conv3x3(N, N),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            conv3x3(N, N, stride=2),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            conv3x3(N, N),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            conv3x3(N, N),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            conv3x3(N, N, stride=2),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            conv3x3(N, N),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            conv3x3(N, N, stride=2),\n",
    "            \n",
    "        )\n",
    "        self.h_s = nn.Sequential(\n",
    "            conv3x3(N, N),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            subpel_conv3x3(N, N, 2),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            conv3x3(N, N),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            subpel_conv3x3(N, N, 2),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            conv3x3(N, N),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            subpel_conv3x3(N, N, 2),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            conv3x3(N, N),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            subpel_conv3x3(N, N, 2),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            conv3x3(N, N),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            subpel_conv3x3(N, N, 2),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            conv3x3(N, dim * 2),        # element-wise\n",
    "#             conv3x3(N, quantizers * 2), # vector-wise\n",
    "        )\n",
    "        self.g_s = nn.Sequential(\n",
    "            AttentionBlock(dim),\n",
    "            ResidualBlock(dim, dim),\n",
    "            ResidualBlockUpsample(dim, N, 2),\n",
    "            AttentionBlock(N),\n",
    "            ResidualBlock(N, N),\n",
    "            ResidualBlockUpsample(N, N, 2),\n",
    "            AttentionBlock(N),\n",
    "            ResidualBlock(N, N),\n",
    "            subpel_conv3x3(N, 3, 2),\n",
    "        )\n",
    "        if quantizers == 1:\n",
    "            self.vq = VectorQuantize(\n",
    "                dim = dim,\n",
    "                codebook_size = CB_size,    # codebook size\n",
    "                decay = 0.99,               # the exponential moving average decay, lower means the dictionary will change faster\n",
    "                commitment_weight = 0.25,   # the weight on the commitment loss\n",
    "                accept_image_fmap = True,\n",
    "            )\n",
    "        elif quantizers > 1 and not Hierarchical_architecture:\n",
    "            self.vq = MultiLayerVQ(\n",
    "                num_quantizers = quantizers,\n",
    "                dim = dim // quantizers,\n",
    "                codebook_size = CB_size,    # codebook size\n",
    "                decay = 0.99,               # the exponential moving average decay, lower means the dictionary will change faster\n",
    "                commitment_weight = 0.25,   # the weight on the commitment loss\n",
    "                accept_image_fmap = True,\n",
    "            )\n",
    "        elif quantizers > 1 and Hierarchical_architecture:\n",
    "            if type(dim_list) != list or type(CB_size) != list:\n",
    "                print(\"HierarchicalVQ's dim & CB size should be list.\")\n",
    "            self.vq = HierarchicalVQ(\n",
    "                num_quantizers = quantizers,\n",
    "                dim_list = dim_list,\n",
    "                CB_size_list = CB_size,    # codebook size\n",
    "                decay = 0.99,               # the exponential moving average decay, lower means the dictionary will change faster\n",
    "                commitment_weight = 0.25,   # the weight on the commitment loss\n",
    "                accept_image_fmap = True,\n",
    "            )            \n",
    "            \n",
    "    def calc_cross_entropy(self, symbol):\n",
    "        ones = torch.ones_like(symbol).to(device).float()\n",
    "        cross_entropy_from_N_01 = self.gaussian_conditional._likelihood(symbol, ones)\n",
    "        cross_entropy_from_N_01 = self.lower_bound_l(cross_entropy_from_N_01)\n",
    "        return cross_entropy_from_N_01\n",
    "\n",
    "    def standardized(self, y, means, scales):\n",
    "        variance = self.lower_bound_s(scales)\n",
    "        if self.quantizers == 1 or element_wise:\n",
    "            y_std = y - means\n",
    "            y_std = y_std / variance\n",
    "        else:\n",
    "            pd = self.dim // self.quantizers # process_dim\n",
    "            for i in range(self.quantizers):\n",
    "                y_ = y[:, i*pd:(i+1)*pd] - means[:, i].unsqueeze(1)\n",
    "                y_ = y_ / variance[:, i].unsqueeze(1)\n",
    "                y_std = y_ if i == 0 else torch.cat([y_std, y_], 1)\n",
    "        ce_N01 = self.calc_cross_entropy(y_std)\n",
    "        return y_std, ce_N01\n",
    "    def destandardized(self, y_std, means, scales):\n",
    "        variance = self.lower_bound_s(scales)\n",
    "        if self.quantizers == 1 or element_wise:        \n",
    "            y_ = y_std * variance\n",
    "            y_ = y_ + means\n",
    "        else:\n",
    "            pd = self.dim // self.quantizers # process_dim\n",
    "            for i in range(self.quantizers):\n",
    "                y_hat = y_std[:, i*pd:(i+1)*pd] * variance[:, i].unsqueeze(1)\n",
    "                y_hat = y_hat + means[:, i].unsqueeze(1)\n",
    "                y_ = y_hat if i == 0 else torch.cat([y_, y_hat], 1)            \n",
    "        return y_\n",
    "\n",
    "    def represent_befor_quantize(self, y):\n",
    "        z = self.h_a(y)\n",
    "        z_hat, z_likelihoods = self.entropy_bottleneck(z)\n",
    "        gaussian_params = self.h_s(z_hat)\n",
    "        scales_hat, means_hat = gaussian_params.chunk(2, 1)\n",
    "        y_std, ce = self.standardized(y, means_hat, scales_hat)\n",
    "        return y_std, ce, z_likelihoods, scales_hat, means_hat\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.g_a(x)\n",
    "        if hyper_enable:\n",
    "            y_std, ce, z_likelihoods, scales_hat, means_hat = self.represent_befor_quantize(y)\n",
    "        else:\n",
    "            y_std = y\n",
    "            ce = self.calc_cross_entropy(y)\n",
    "            z_likelihoods = torch.tensor(0)\n",
    "        \n",
    "        y_hat, id, commit, usage = self.vq(y_std)  # (b, Q, w, h), (b, Q, w, h), (b), (b)\n",
    "        y_hat_ = self.destandardized(y_hat, scales_hat, means_hat) if hyper_enable else y_hat\n",
    "        x_hat = self.g_s(y_hat_)\n",
    "\n",
    "        return {\n",
    "            \"x_hat\": x_hat,\n",
    "            \"likelihoods\": {\"z\": z_likelihoods},\n",
    "            \"commit\": commit,\n",
    "            \"usage\": usage,\n",
    "            \"cross\": ce,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_psnr(a, b):\n",
    "    mse = torch.mean((a - b)**2).item()\n",
    "    return -10 * math.log10(mse)\n",
    "\n",
    "def compute_mse(a, b):\n",
    "    mse = torch.mean((a - b)**2).item()\n",
    "    return mse\n",
    "\n",
    "def compute_msssim(a, b):\n",
    "    return ms_ssim(a, b, data_range=1.).item()\n",
    "\n",
    "def compute_bpp(out_net):\n",
    "    size = out_net['x_hat'].size()\n",
    "    num_pixels = size[0] * size[2] * size[3]\n",
    "    return sum(torch.log(likelihoods).sum() / (-math.log(2) * num_pixels)\n",
    "              for likelihoods in out_net['likelihoods'].values()).item()\n",
    "class AverageMeter:\n",
    "    \"\"\"Compute running average.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        \n",
    "def configure_optimizers(net):\n",
    "    \"\"\"Separate parameters for the main optimizer and the auxiliary optimizer.\n",
    "    Return two optimizers\"\"\"\n",
    "    parameters = {\n",
    "        n\n",
    "        for n, p in net.named_parameters()\n",
    "        if not n.endswith(\".quantiles\") and p.requires_grad and not n.startswith(\"h\")\n",
    "    }\n",
    "    aux_parameters = {\n",
    "        n\n",
    "        for n, p in net.named_parameters()\n",
    "        if n.endswith(\".quantiles\") and p.requires_grad\n",
    "    }\n",
    "    hyper_parameters = {\n",
    "            n\n",
    "        for n, p in net.named_parameters()\n",
    "        if n.startswith(\"h\") and p.requires_grad\n",
    "    }\n",
    "\n",
    "    # Make sure we don't have an intersection of parameters\n",
    "    params_dict = dict(net.named_parameters())\n",
    "    inter_params = parameters & aux_parameters & hyper_parameters\n",
    "    union_params = parameters | aux_parameters | hyper_parameters\n",
    "\n",
    "    assert len(inter_params) == 0\n",
    "    assert len(union_params) - len(params_dict.keys()) == 0\n",
    "\n",
    "    optimizer = optim.Adam(\n",
    "        (params_dict[n] for n in sorted(parameters)),\n",
    "        lr=1e-4,\n",
    "    )\n",
    "    hyper_optimizer = optim.Adam(\n",
    "        (params_dict[n] for n in sorted(hyper_parameters)),\n",
    "        lr=1e-4,\n",
    "    )\n",
    "    aux_optimizer = optim.Adam(\n",
    "        (params_dict[n] for n in sorted(aux_parameters)),\n",
    "        lr=1e-3,\n",
    "    )\n",
    "    return optimizer, hyper_optimizer, aux_optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function, Train, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "class RateDistortionLoss(nn.Module):\n",
    "    \"\"\"Custom rate distortion loss with a Lagrangian parameter.\"\"\"\n",
    "\n",
    "    def __init__(self, lmbda=1e-2):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.lmbda = lmbda\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        N, _, H, W = target.size()\n",
    "        out = {}\n",
    "        num_pixels = N * H * W\n",
    "        out[\"mse_loss\"] = self.mse(output[\"x_hat\"], target)\n",
    "        out[\"commit\"] = output[\"commit\"]\n",
    "        out[\"perplexity\"] = output[\"usage\"]\n",
    "        if hyper_enable:        \n",
    "            out[\"bpp_loss\"] = sum(\n",
    "                (torch.log(likelihoods).sum() / (-math.log(2) * num_pixels))\n",
    "                for likelihoods in output[\"likelihoods\"].values()\n",
    "            )\n",
    "        else:\n",
    "            out[\"bpp_loss\"] = 0\n",
    "        out[\"ce_loss\"] = torch.log(output[\"cross\"]).sum() / (-math.log(2) * num_pixels)\n",
    "        if ce_loss_enable:\n",
    "            out[\"loss\"] = self.lmbda * 255**2 *  out[\"mse_loss\"] + out[\"commit\"].sum() + out[\"ce_loss\"]\n",
    "        else:\n",
    "            out[\"loss\"] = self.lmbda * 255**2 *  out[\"mse_loss\"] + out[\"commit\"].sum()\n",
    "        return out\n",
    "\n",
    "def train_one_epoch(\n",
    "    model, criterion, train_dataloader, optimizer, hyper_optimizer, aux_optimizer, epoch, clip_max_norm\n",
    "):\n",
    "    model.train()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    for i, d in enumerate(train_dataloader):\n",
    "        d = d.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        hyper_optimizer.zero_grad()\n",
    "        aux_optimizer.zero_grad()\n",
    "        out_net = model(d)\n",
    "        \n",
    "        out_criterion = criterion(out_net, d)\n",
    "        out_criterion[\"loss\"].backward()\n",
    "        optimizer.step()\n",
    "        if hyper_enable:\n",
    "            hyper_optimizer.step()\n",
    "        aux_loss = model.aux_loss()\n",
    "        aux_loss.backward()\n",
    "        aux_optimizer.step()\n",
    "\n",
    "        if i == 100:\n",
    "            print(\n",
    "                f\"Train epoch {epoch}: [\"\n",
    "                f\"{i*len(d)}/{len(train_dataloader.dataset)}\"\n",
    "                f\" ({100. * i / len(train_dataloader):.0f}%)]\"\n",
    "                f'\\t Loss: {out_criterion[\"loss\"].item():.3f} |'\n",
    "                f'\\t MSE loss: {out_criterion[\"mse_loss\"].item():.3f} |'\n",
    "                f'\\t Bpp loss: {out_criterion[\"bpp_loss\"].item():.2f} |'                \n",
    "                f'\\t vq commit:{out_criterion[\"commit\"].sum().item():.2f} |'\n",
    "                f'\\t perplexity:{out_criterion[\"perplexity\"].mean().item():.2f} |'\n",
    "                f\"\\t Aux loss: {aux_loss.item():.2f}\"\n",
    "            )\n",
    "            \n",
    "def test_epoch(epoch, test_dataloader, model:Hyper_VQ, criterion):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    loss = AverageMeter()\n",
    "    psnr_score = AverageMeter()\n",
    "    mse_loss = AverageMeter()\n",
    "    bpp_loss = AverageMeter()\n",
    "    aux_loss = AverageMeter()\n",
    "    y_mse = AverageMeter()\n",
    "    commit_loss = AverageMeter()\n",
    "    usage_status = AverageMeter()\n",
    "    CE_loss = AverageMeter()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for d in test_dataloader:\n",
    "            d = d.to(device)\n",
    "\n",
    "            y = model.g_a(d)\n",
    "            if hyper_enable:\n",
    "                y_std, ce, z_likelihoods, scales_hat, means_hat = model.represent_befor_quantize(y)\n",
    "            else:\n",
    "                y_std = y\n",
    "                ce = model.calc_cross_entropy(y)\n",
    "                z_likelihoods = torch.tensor(0)\n",
    "            \n",
    "            y_hat, id, commit, usage = model.vq(y_std)  # (b, Q, w, h), (b, Q, w, h), (b), (b)\n",
    "            y_hat_ = model.destandardized(y_hat, scales_hat, means_hat) if hyper_enable else y_hat\n",
    "            x_hat = model.g_s(y_hat_)\n",
    "\n",
    "            out_net = {\n",
    "                \"x_hat\": x_hat,\n",
    "                \"likelihoods\": {\"z\": z_likelihoods},\n",
    "                \"commit\": commit,\n",
    "                \"usage\": usage,\n",
    "                \"cross\": ce,\n",
    "            }\n",
    "\n",
    "            out_criterion = criterion(out_net, d)\n",
    "            loss.update(out_criterion[\"loss\"])\n",
    "            psnr_score.update(compute_psnr(x_hat, d))\n",
    "            mse_loss.update(out_criterion[\"mse_loss\"])\n",
    "            bpp_loss.update(out_criterion[\"bpp_loss\"])\n",
    "            aux_loss.update(model.aux_loss())\n",
    "            y_criterion = compute_mse(y_hat_, y)\n",
    "            y_mse.update(y_criterion)\n",
    "            commit_loss.update(out_criterion[\"commit\"].sum())\n",
    "            usage_status.update(out_criterion[\"perplexity\"].mean())\n",
    "            CE_loss.update(out_criterion[\"ce_loss\"])\n",
    "                \n",
    "#     print(\n",
    "#         f\"{epoch}: \"\n",
    "#         f\"\\tLoss: {loss.avg:.3f} |\"\n",
    "#         f\"\\tPSNR: {psnr_score.avg:.3f} |\"\n",
    "#         f\"\\tMSE: {mse_loss.avg:.3f} |\"\n",
    "#         f\"\\ty_mse : {y_mse.avg:.2f} |\"\n",
    "#         f\"\\tcommit : {commit_loss.avg:.2f} |\"\n",
    "#         f\"\\tusage : {usage_status.avg:.2f} |\"\n",
    "#         f\"\\tCEL : {CE_loss.avg:.2f} |\"\n",
    "#         f\"\\tBpp: {bpp_loss.avg:.2f} |\"\n",
    "#     )\n",
    "    with open(f'{log_path}_{loop}.csv','a') as f:\n",
    "        log_s = f\"{epoch}, {psnr_score.avg}, {mse_loss.avg}, {y_mse.avg}, {commit_loss.avg}, {usage_status.avg}, {CE_loss.avg}, {bpp_loss.avg}\\n\"\n",
    "        f.write(log_s)\n",
    "    return loss.avg\n",
    "\n",
    "def eval_image(net):\n",
    "    device = next(net.parameters()).device\n",
    "    img = Image.open('../assets/kodim15.png').convert('RGB')\n",
    "    x = transforms.ToTensor()(img).unsqueeze(0).to(device = device)\n",
    "    with torch.no_grad():\n",
    "        out_net = net.forward(x)\n",
    "    out_net['x_hat'].clamp_(0, 1)\n",
    "    psnr = compute_psnr(x, out_net[\"x_hat\"])\n",
    "    msssim = compute_msssim(x, out_net[\"x_hat\"])\n",
    "    if hyper_enable:\n",
    "        bitrate = compute_bpp(out_net)\n",
    "    else:\n",
    "        bitrate = 0\n",
    "    log_s = f\"{psnr}, {msssim}, {bitrate}\\n\"\n",
    "    print(log_s)\n",
    "    torchvision.utils.save_image(out_net['x_hat'], f'{loop}.png')\n",
    "    return psnr, msssim, bitrate\n",
    "\n",
    "def save_checkpoint(state, is_best, filename=f'{version}.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, \"best.vq.tar\")\n",
    "\n",
    "def get_CE(symbol):\n",
    "    CE = net.calc_cross_entropy(symbol)\n",
    "    CE_avg = torch.log(CE).sum() / (-math.log(2) * (b * h * w))\n",
    "    return CE_avg.item()\n",
    "\n",
    "def get_mean_and_sigma(symbol):\n",
    "    symbol_flat = symbol.view(-1)    \n",
    "    mean = symbol_flat.mean()\n",
    "    mean_flat = torch.zeros_like(symbol_flat) + mean    \n",
    "    sigma = ((symbol_flat - mean_flat) ** 2).mean()\n",
    "    return mean, sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose(\n",
    "    [transforms.RandomCrop((256, 256)), transforms.ToTensor()]\n",
    ")\n",
    "\n",
    "test_transforms = transforms.Compose(\n",
    "    [transforms.ToTensor()]\n",
    ")\n",
    "\n",
    "train_dataset = ImageFolder('../../../DIV2K_HR', split=\"train\", transform=train_transforms)\n",
    "test_dataset = ImageFolder('../../../DIV2K_HR', split=\"test\", transform=test_transforms)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size = batch_size_setting,\n",
    "    num_workers = workers_setting,\n",
    "    shuffle=True,\n",
    "    pin_memory=(device == \"cuda\"),\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size = 18,\n",
    "    num_workers = workers_setting,\n",
    "    shuffle=False,\n",
    "    pin_memory=(device == \"cuda\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantizers_list = [4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
    "bpp_base_list = [36, 32, 28, 24, 20, 16, 12, 8, 4]\n",
    "\n",
    "# non-hierarchical\n",
    "# CB_size_list = [512, 256, 128, 64, 32, 16, 8, 4, 2] \n",
    "# dim = 64 \n",
    "\n",
    "# hierarchical\n",
    "CB_size_list = []\n",
    "item = 512\n",
    "for i in range(times):\n",
    "    list_temp = [item, item, item, item]\n",
    "    item = item // 2\n",
    "    CB_size_list.append(list_temp)\n",
    "# dim = 64 # non-hierarchical\n",
    "dim = [8, 8, 16, 32] \n",
    "\n",
    "for l in range(times):\n",
    "    loop = l\n",
    "    net = Hyper_VQ(128, quantizers=quantizers_list[loop], CB_size=CB_size_list[loop], dim=dim).to(device)\n",
    "    optimizer, hyper_optimizer, aux_optimizer = configure_optimizers(net)\n",
    "    lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"min\")\n",
    "    criterion = RateDistortionLoss(lmbda=lambda_setting)\n",
    "\n",
    "    path = 'vq.tar'\n",
    "    last_epoch = 0\n",
    "    if False:\n",
    "        print(\"Loading\", path)\n",
    "        checkpoint = torch.load(path, map_location=device)\n",
    "        last_epoch = checkpoint[\"epoch\"] + 1\n",
    "        net.load_state_dict(checkpoint[\"state_dict\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "        aux_optimizer.load_state_dict(checkpoint[\"aux_optimizer\"])\n",
    "        lr_scheduler.load_state_dict(checkpoint[\"lr_scheduler\"])\n",
    "\n",
    "    with open(f'{log_path}_{loop}.csv','a') as f:\n",
    "        log_s = f\"epochs, PSNR, MSE, y_mse, commit, usage, CE, bpp\\n\"\n",
    "        f.write(log_s)\n",
    "\n",
    "    best_loss = float(\"inf\")\n",
    "    for epoch in range(last_epoch, epochs):\n",
    "        lring = optimizer.param_groups[0]['lr']\n",
    "        if lring < 1e-7:\n",
    "            break\n",
    "#         print(f\"Learning rate: {lring}\")\n",
    "        train_one_epoch(\n",
    "            net,\n",
    "            criterion,\n",
    "            train_dataloader,\n",
    "            optimizer,\n",
    "            hyper_optimizer,\n",
    "            aux_optimizer,\n",
    "            epoch,\n",
    "            1,\n",
    "        )\n",
    "        loss = test_epoch(epoch, test_dataloader, net, criterion)\n",
    "        lr_scheduler.step(loss)\n",
    "\n",
    "        is_best = loss < best_loss\n",
    "        best_loss = min(loss, best_loss)\n",
    "\n",
    "        save_checkpoint(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"state_dict\": net.state_dict(),\n",
    "                \"loss\": loss,\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "                \"aux_optimizer\": aux_optimizer.state_dict(),\n",
    "                \"lr_scheduler\": lr_scheduler.state_dict(),\n",
    "            },\n",
    "            is_best,\n",
    "        )\n",
    "\n",
    "    psnr, msssim, zbpp = eval_image(net)\n",
    "    model = net\n",
    "    with torch.no_grad():\n",
    "        for d in test_dataloader:\n",
    "            d = d.to(device)\n",
    "\n",
    "            y = model.g_a(d)\n",
    "            if hyper_enable:\n",
    "                y_std, ce, z_likelihoods, scales_hat, means_hat = model.represent_befor_quantize(y)\n",
    "            else:\n",
    "                y_std = y\n",
    "                ce = model.calc_cross_entropy(y)\n",
    "                z_likelihoods = torch.tensor(0)\n",
    "\n",
    "            y_hat, id, commit, usage = model.vq(y_std)  # (b, Q, w, h), (b, Q, w, h), (b), (b)\n",
    "            y_hat_ = model.destandardized(y_hat, scales_hat, means_hat) if hyper_enable else y_hat\n",
    "            x_hat = model.g_s(y_hat_)\n",
    "\n",
    "    x_hat_ =  x_hat.clamp(0, 1)\n",
    "    print(compute_psnr(x_hat_, d))        \n",
    "    b, c, h, w = d.shape\n",
    "    m, s = get_mean_and_sigma(y)\n",
    "    m_, s_ = get_mean_and_sigma(y_std)\n",
    "\n",
    "    if not hyper_enable:   # VQVAE\n",
    "        total_bpp = bpp_base_list[loop] / 8 / 8\n",
    "    elif hyper_enable:     # Hyper-VQ\n",
    "        total_bpp = bpp_base_list[loop] / 8 / 8 + zbpp\n",
    "        \n",
    "    log_s = f\",mse wt, {lambda_setting}\\n\"\n",
    "    log_s = log_s + f\",yCE, {get_CE(y)}\\n\"\n",
    "    log_s = log_s + f\",y_CE, {get_CE(y_std)}\\n\"\n",
    "    log_s = log_s + f\",ymu, {m}\\n\"\n",
    "    log_s = log_s + f\",ysigma, {s}\\n\"\n",
    "    log_s = log_s + f\",y_mu, {m_}\\n\"\n",
    "    log_s = log_s + f\",y_sigma, {s_}\\n\"\n",
    "    log_s = log_s + f\",PSNR, {psnr}\\n\"\n",
    "    log_s = log_s + f\",ms-ssim, {msssim}\\n\"\n",
    "#     log_s = log_s + f\",usage, {usage.item()}\\n\"\n",
    "    log_s = log_s + f\",zbpp, {zbpp}\\n\"\n",
    "    log_s = log_s + f\",BPP, {total_bpp}\\n\"\n",
    "    \n",
    "    usage_list = []\n",
    "    mse_list = []\n",
    "\n",
    "    for i in range(4):    \n",
    "        start = 0 if i == 0 else sum(dim[:i])\n",
    "        end = dim[0] if i == 0 else sum(dim[:i+1])\n",
    "        mse = F.mse_loss(y[:, start:end], y_hat_[:, start:end])    \n",
    "        mse_list.append(mse.item())\n",
    "        usage_list.append(usage[i].item())\n",
    "\n",
    "    log_s = log_s+ f\",usage,\"\n",
    "    for i in range(4):\n",
    "        log_s = (log_s + f\"{usage_list[i]},\") \n",
    "    log_s = log_s + f\"\\n,part_mse,\"\n",
    "    for i in range(4):\n",
    "        log_s = (log_s + f\"{mse_list[i]},\" )\n",
    "    \n",
    "    print(log_s)\n",
    "    with open(f'{log_path}_{loop}.csv','a') as f:\n",
    "        f.write(log_s)        \n",
    "        \n",
    "    print(f'{loop}: {psnr} in {total_bpp}.')\n",
    "    print(f'---')\n",
    "    print(f'\\n')    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "4b3695a85a4caa0373e4268d79926f2001c5518f41e76a28112350d8a4ba6e82"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
